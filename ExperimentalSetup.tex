\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Method}
\section{Experimental Infrastructure}

\subsection{Source-Testing Facility}

The Source Testing Facility (STF) at the Division of Nuclear Physics in Lund, Sweden, is a fully equipped user facility for characterization of detectors, shielding and sources~\cite{Messi}.
The STF is a collaboration between the Division of Nuclear Physics and the ESS Detector Group, it is employed for development of detectors for the ESS and industry. By using actinide/Be fast neutron sources as well as gamma-ray sources the STF offers easy and reliable access to fast neutrons and gamma-rays. Additionally, the neutrons can be moderated to lower energies with various materials. The STF is divided into two areas, see Fig. \ref{fig:STF}. The user area contains data-acquisition systems (DAQ), workstations as well as a wide range of electronics modules and detectors. The smaller interlocked area is where measurements are carried out. This area contains an array of shielding materials as well as a dedicated neutron tagging setup known as the Aquarium.

\begin{figure}[ht]
	\center
    	\includegraphics[width=0.7\textwidth]{AnalogSetup/STF.pdf}
	\caption[The Source-Testing Facility.]{The Source-Testing Facility. The smaller room to the left is the interlocked area, where measurements are carried out. The user area area to the right contains DAQ setups, workstations and storage \cite{Messi}.}
	\label{fig:STF}
\end{figure}

\subsection{The Aquarium}
The fast-neutron source is located inside a 140$\times$140$\times$140 \si{\cm}${}^\text{3}$ tank of water referred to as the Aquarium, see Fig. \ref{fig:aquarium}. The Aquarium has four horizontal cylindrical beam ports, intersecting at a central volume. The source and four gamma-ray detectors, may be placed within the central volume.  The beam ports and the central volume are air filled, allowing neutrons and gamma-rays to reach a neutron/gamma-ray detector placed on the other side. Each of the beam ports can be plugged when not in use.
By moderating fast neutrons as well as absorbing some of them via the $^{\text{1}}$H($n$,$\gamma$)$^{\text{2}}$H$^*$ reaction, the water tank both provides shielding from the sources and gives rise to a distinguishable gamma-ray energy of \SI{2.23}{MeV} produced in the de-excitation of the deuteron. This peak will be useful later when performing energy calibrations of the detectors, see Sec. \ref{sec:Ecal}. 
An actinde/Be source may be positioned on the central vertical axis of the Aquarium and can be raised to the same height as the ports for ToF measurements. In a ``lowered" parked position, there is no direct line-of-sight through air from source through the ports. The four gamma-ray detectors are located near the source, but are raised slightly to allow a direct line of sight from source through the ports, see Fig. \ref{fig:aquarium}.
\begin{figure}[ht]
	\center
    	\includegraphics[width=\textwidth]{AnalogSetup/aquarium_combined.pdf}
	\caption[The Aquarium]{The Aquarium. Left panel: Oblique view with neutron and gamma-ray detector in front of one of the horizontal cylindrical beam ports. Lines-of-flight from source and out of two of the ports are indicated with arrows. The source and the four gamma-ray detectors are located in the tubes at the center of the aquarium. Right panel: Top view.}
	\label{fig:aquarium}
\end{figure}

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/setup_flowchart.pdf}
        \caption[Schematic of experimental setup]{Schematic of the experimental setup. In green the digital setup, in yellow the analog setup.}
    \label{fig:setup}
\end{figure}


\subsection{Radiation Sources}
Two radiation sources were used in this work. The actinide/Beryllium fast neutron and gamma-ray source $^\text{238}\text{Pu}/^\text{9}\text{Be}$ was used for tagging fast neutrons and for energy calibration of the neutron/gamma-ray detector. As shown in Sec. \ref{sec:freeNeutrons} the PuBe source produces both a cascade of low-energy photons and fast neutrons which 55\% of the time are accompanied by the emission of a \SI{4.44}{\MeV} gamma-ray. The source has been measured to produce approximately $\text{2.99}\cdot\text{10}^\text{6}$ neutrons per second \cite{Scherzinger:2017}. The pure gamma-ray source $^\text{60}\text{Co}$ was used purely for detector calibration. $^\text{60}\text{Co}$ decays to excited states of $^\text{60}\text{Ni}$ via beta decay. De-excitation releases gamma-rays of either 1.17 or \SI{1.33}{MeV} \cite{Nudat}.



\subsection{Fast Neutron and Gamma-Ray Detectors}
As ToF depends on the accurate timing of \textgamma /n and \textgamma /\textgamma\; pairs, it is essential to detect both particle species with accurate timing. In this work, two different detectors were used for detecting neutrons and gamma-rays from the sources. A liquid organic NE213 detector was used to detect both gamma-rays and neutrons while a Cerium doped Yttrium Aluminum Perovskite crystal (YAP) detector was used to detect gamma-rays. 

Since its introduction in the early 1960s, the NE213 liquid organic scintillator has become the gold standard for fast-neutron detection due to its excellent neutron/gamma-ray discrimination capabilities and detection efficiency. 
The drawbacks of this scintillator are that it is toxic and highly volatile with a flash point of 26$\text{\degree}$C. The NE213 used here was produced by Nuclear Enterprises, but is equivalent to EJ301, currently produced by Eljen Technology \cite{Eljen}. The decay times of the first three scintillation components are \SI{3.16}{ns}, \SI{32.3}{ns} and \SI{270}{ns} \cite{Eljen}. It is contained in a 122$\times$122$\times$179 \si{\mm}$^\text{3}$ volume, which is connected to a photomultiplier tube via a lightguide\footnote{This detector was constructed by Johan Sj√∂green as part of his thesis work 2009-2010 \cite{sjogren}.}.

Near the source, four YAP detectors are placed. The YAP detectors are composed of a Cerium doped Yttrium Aluminum Perovskite crystal mounted on a photomultiplier tube. These inorganic scintillators are largely insensitive to neutrons and provide excellent timing of gamma-rays. As the YAP detectors are located closer to the source they experience a significantly higher flux than the NE213 detector. This makes time resolution and decay time critical factors in their performance. The metastable states of the YAP pulses have decay times of around 27 ns, but individual high amplitude pulses may be several hundred nanoseconds long, which means that the detector can handle count rates in the low MHz range. To simplify the analysis and limit the data rates only one YAP detector was used.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/detectors.pdf}
        \caption[Photographs of detectors]{Photographs of detectors. Left: NE213 detector. Right: YAP detector}
    \label{fig:detectors}
\end{figure}


\section{Signal Processing}
Two different DAQ systems were used. Both experimental setups differ only in the DAQ system used and share the same physical setup of detectors, shielding and radiation source. By sending the detector signals through an active splitter both DAQs can even be run in parallel on the same input signals. The first setup employed NIM modules to process signals and generate a trigger decision. VME modules were used to digitize the timing and charge characteristics of the signals. The digitized data was transferred to a computer, where it was saved and plotted in real time. Since this setup does most of the data processing via analog electronics it will be referred to as the \textit{analog setup}. 

The second setup is based on a digitizer which records detector signals as digital waveforms for offline analysis. Since all of the processing of the signals is performed digitally, this setup will be referred to as the \textit{digital setup}. 

Figure \ref{fig:DAQ} shows the analog rack on the left and the digitizer to the right. The analog setup is composed of a variety of NIM and VME modules as well as meters of LEMO cables. The digital setup is composed of a single digitizer, which takes two raw detector signals as inputs and fits in a single VME slot.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/DAQ.pdf}
        \caption[Data acquisition systems.]{Data acquisition systems. The analog rack is shown to the left. Digitization modules are highlighted by the orange arrow, and the detectors power supply is indicated by the blue arrow. A closeup of the analog electronics is indicated in red. To the right, highlighted in green is the digitizer.}
    \label{fig:DAQ}
\end{figure}

\subsection{Analog}
The detector signals are copied by a Fan-in-fan-out (FIFO) module and copies are sent on to both the analog and the digital DAQ setups as shown shown in Fig. \ref{fig:setup}. On the analog side the signals are processed using a number of NIM and VME modules before finally time and energy information is sent through optical link to a computer where it is written to the hard drive. 

\subsubsection{Discriminators}
The YAP and NE213 pulses are sent to constant-fraction discriminators (CFD). The CFDs are labeled \circled{1} in Fig. \ref{fig:setup}. The motivation for using this device is that ToF measurements require high timing precision in the start and stop signals. Simply triggering on the leading edge of pulses will lead to time walk for similarly shaped pulses of varying amplitudes. Time walk means that pulses of the same shape but different amplitude will result in triggers at different times, see Fig. \ref{fig:discriminator}. Here the smaller pulse passes the threshold at a later time than the larger pulse.

By instead triggering on the point where a pulse reaches a certain fraction of its peak amplitude, time walk can be nearly eliminated \cite{Leo}. This may be achieved by dividing or copying the signal, inverting one copy and delaying the other one. The pulses are then added and the zero crossing of the summed signal corresponds to a fraction of the input pulse, see Fig. \ref{fig:cfd}. Changing the amplitude of the incoming signal will have the same effect on both copies of the signal, and consequently the CFD trigger time is amplitude independent. The output of the constant fraction discriminator is a logic pulse representing the time of the CFD triggering. The CFD module also eforces an amplitude threshold of \SI{94.6}{mV} for the NE213 detector \SI{25.0}{mV} for the YAP detector. Only pulses above the thresold generate a logic pulse.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.7\textwidth]{AnalogSetup/threshold.pdf}
        \caption[Time walk resulting from leading edge triggering]{Time walk resulting from leading edge triggering. Figure adapted from Ref. \cite{rofors}.}
    \label{fig:discriminator}
\end{figure}

\begin{figure}[h]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogSetup/cfd_emil.pdf}
        \caption[CFD trigger principle]{Illustration of the principle of an analog CFD. Figure from Ref. \cite{rofors}.}
    \label{fig:cfd}
\end{figure}



\subsubsection{Trigger logic}
The NE213 logic pulses generated by this CFD are sent to a latch (labeled \circled{2} in Fig. \ref{fig:setup}), which lets through this single pulse and blocks further pulses from passing until it has received a reset signal. The pulses that make it through are passed on to the data acquisition modules presented below. The reset signal is given after \SI{10}{\micro\second}, which gives the data-acquisition modules (see below) sufficient time to digitize the data. The amount of time the latch is closed is called deadtime, because the system is unable to process new events during this time. The deadtime is measured by an uninhibited and an inhibited scaler (a scaler is a device for couning), connected to an oscillator. The oscillator increments the scalers with at a specific frequency, but whenever the latch is closed the inhibited scaler is no longer incremented. This makes it possible to calculate the livetime as the ratio of inhibited scaler count to the uninhibited scaler count.

Although the setup contains both YAP and NE213 detectors only the NE213 is used to trigger acquisition. This is because the NE213 will trigger less often than the YAP, due to covering a smaller solid angle. This way the amount of time the system spends processing uninteresting events is minimized.

\subsubsection{Charge to digital converters}
The pulses that make it through the latch are used to generate \SI{60}{\ns} and \SI{500}{\ns} logic pulses (with the first \SI{25}{\ns} preceding the CFD trigger point), which act as integration windows or gates for the charge-to-digital converters (QDCs). In Fig. \ref{fig:setup}, these modules are labeled \circled{3}.

Copies of the analog current pulses are also sent to the QDC modules, and each module carries out an integration for the duration of the gate. This charge provides a measure of the energy deposited in the detector by the pulse on two different timescales. The QDC modules send their outputs via optical link to a computer, where they are written to the hard drive, see Fig. \ref{fig:setup}.

The charge integration performed by the QDC modules is a measure of the energy deposited in a detector. In the NE213 detector neutrons primarily interact via scattering from $^1$H while gamma-rays primarily interact with atomic electrons. It is customary to calibrate a fast neurtron detector with gamma-ray sources, employing the electron equivalent energy. This is the energy an electron would need to have had in order to produce the same amount of charge in the detector. One way of performing this energy calibration is the Knox method of examining the compton edge corresponding to gamma-ray sources \cite{Nilsson}. The maximum energy transferred by a gamma-ray to a recoil electron, $(E_e)_{max}$, is given by:
\begin{equation}
	(E_{e})_{max}\;=\;\frac{2E\gamma^2}{m_e + 2E_\gamma} \;[MeV_{ee}]
\end{equation}
Where $E_\gamma$ is the energy of the gamma-ray and $m_e$ is the electrons mass.
Next a Gaussian is fitted to the region of the Compton edge. The QDC channel where the Gaussian distribution reaches 89\% of its height is associated with $(E_e)_{max}$. The \SI{4.44}{MeV} Compton edge produced by the deexcitation $^{12}$C was used as well as the \SI{2.23}{MeV} Compton edge produced by the de-excitation of a deutoron produced.
 The narrow peak the furthest to the left in Fig. \ref{fig:qdc_a} is the pedestal. It is produced when the QDC is made to trigger when there there is no current pulse. The pedestal represents the zero point of the QDC spectrum so for the energy calibration it is associated with \SI{0}{MeV_{ee}}. All the values used in the calibration are shown in table \ref{tab:knox_a}.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	QDC(channel)             & 67.5 & 11.43 & 2718 \\
	\hline
	E(MeV)          & 0    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(\textrm{MeV}_\textrm{{ee}})$ & 0    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
   	\captionsetup{width=0.435\linewidth}
	\caption[Analog energy calibration data.]{Analog energy calibration data.}
	\label{tab:knox_a}
\end{table}

QDC spectrum is shown in Fig. \ref{fig:qdc_a} top panel. The x-axis at the top of the panel shows the calibrated energy scale. The narrow peak located to the far left is the pedestal. The bump immediately to the right of it is produced when this YAP trigger by chance coincides with something in the NE213 detector.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogResults/Ecall.pdf}
        \caption[Energy calibration of the analog setup]{Top panel: A PuBe QDC spectrum from the NE213 detector. The upper x-axis has been calibrated. Bottom panel: The calibration fit produced with the Knox method. The pedestal is indicated with a red box in both plots.}
    \label{fig:qdc_a}
\end{figure}

The \SI{2.23}{MeV} and the \SI{4.44}{MeV} Compton edges have been highlighted in green and red. Using the Knox method a set of points correlating the energy deposition in QDC channels to \si{\MeV}$_\text{ee}$. The points are also shown in table \ref{tab:knox_a}. Using these points a linear fit was made and plotted in Fig. \ref{fig:qdc_a} bottom panel. With this fit the QDC spectrum was converted to \si{\MeV}$_\text{ee}$. It can be seen that the uncertainty is greatest for the 2.23 \si{\MeV} Compton edge\footnote{The uncertainty was calculated from the Gaussian fits with standard deviation $k$, center ${x}_0$ using error propagation: $\sigma_{channel}\;=\;\sqrt{\sigma_{x_0}^2 + \sigma_{k}^2 4\left(\ln0.89\right)^2 }$}

\subsubsection{Time-to-digital converters}
A logic signal is also sent from the latch to a time-to-digital converter (TDC). This module is labeled \circled{4} in Fig. \ref{fig:setup}. The TDC starts charging a capacitor with a constant current once the start signal is received, and stops again once the stop signal is received. The start signal is the event logic pulse from the NE213 and the stop signal is a delayed event pulse from the YAP. The capacitor then discharges and the amplitude of the discharge pulse is proportional to the time between the start and stop \cite{CAENTDC}. An analog-to-digital converter digitizes the pulse and sends the resulting number to a computer via optical link.
\begin{figure}[h]
	\centering
    	\includegraphics[width=0.8\textwidth]{AnalogSetup/Tcal.pdf}
        \caption[TDC calibration spectrum.]{TDC calibration spectrum. Top panel: Each peak represents a different delay value for the stop pulse. Bottom: A linear fit applied to the TDC and delay values.}
	    \label{fig:Tcal} 
\end{figure}
The raw values provided by the TDC are given in units of TDC channels, so a calibration was necessary in order to convert them into nanoseconds. By triggering the start of the TDC with the same signal used to stop the TDC a very sharp peak was produced. This peak was then shifted in known intervals by increasing cable length in order to produce the calibration fit shown in Fig. \ref{fig:Tcal}. Since the location of $t=0$ is arbitrary the important result is the coefficient \SI{0.28}{\nano\second/channel}, which can be used to convert from TDC channels to nanoseconds.


\subsection{Digital signal processing}
\subsubsection{Digitizer Specifications and Configuration}
A digitizer is an electronic device which converts analog signals to digital waveforms. A continuous analog signal is approximated by a list of numbers, each representing a single sampling point, along with some additional information such as a timestamp based on a global clock and the channel number of the input signal. An acquisition is triggered when the amplitude of the input signal crosses a configurable threshold value.
By this definition a digitizer is not much different from a digital oscilloscope. The main difference is that the oscilloscope has a display and is optimized for portability and realtime diagnostic use, whereas the digitizer is optimized for efficient high-rate data transmission to a computer where further analysis can be carried out, either online or more commonly offline.
The advantage of a digitizer over a traditional analog DAQ system is that it allows the user to process a single data set in multiple ways in order to optimize parameters of the acquisition, without having to change anything in the physical setup or acquire subsequent data. The disadvantage is that the quality of the discretization of a continuous signal is limited by the sampling rate, resolution and bandwidth of the digitizer as well as the data rate of the read-out system.

\begin{figure}[ht]
	\centering
    	\includegraphics[width=0.8\textwidth]{DigitalSetup/samplingrate.pdf}
        \caption[Sampling rate.]{Sampling rate. Illustration of an analog signal sampled with two different sampling rates.}
	    \label{fig:samplingrate} 
\end{figure}

%Sampling rate
The digitizer used in this work is an 8 channel CAEN VX1751 waveform digitizer. It has a sampling rate of \si{1\giga\sample\per\second} in standard mode, but can also be operated in double-edge sampling mode, which disables 4 cannels but increases the sampling rate to \SI{2}{\giga\sample/\second} \cite{CAEN}. The data presented in this thesis was acquired in standard mode. This results in one data point per ns when digitizing signals. Given a set of samples, there will always be an infinite number of waveforms that fit the points \cite{Spectrum}. This is called aliasing. However, if the sampling rate is at least twice as large as the highest frequency component of the signal, aliasing may be avoided. This is known as the Nyquist's Theorem \cite{Spectrum}. Typically an even higher sampling rate is desirable. If the sampling rate is too low important features, such as peak location and amplitude will be less well defined, see Fig \ref{fig:samplingrate}.
In order to reproduce signals accurately, it is not enough to have a high sampling rate. The dynamic range, resolution and bandwidth also need to be considered. 
%resolution
The VX1751 digitizer has a \SI{1}{\volt} dynamic range, which means that the difference between maximum and minimum voltage is \SI{1}{\volt}. This is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bits, so the \si{1\volt} dynamic range is divided into 1024 bins each of size \SI{0.978}{\milli\volt} \cite{CAEN}.

%bandwidth
The VX1751 has a bandwidth of \SI{500}{\mega\hertz}. The bandwidth is important because it determines the frequency range of signals that can be read without significant attenuation. If the frequency components (as obtained from Fourier decomposition of the pulse) of a signal are too high, then the amplitude recorded by the digitizer will be lower than the actual amplitude of the input signal. The following relation offers a rule of thumb for evaluating what bandwidth a given signal type will need:
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{\textrm{rise}}
\end{equation}
Where $T_\textrm{rise}$ is the signals risetime (the time it takes the pulse to rise from 10\% to 90\% of peak amplitude) and $B$ is the bandwidth for which the signal is attenuated to only 70\% of it's orignal amplitude \cite{Leo}. Thus, with a \SI{500}{MHz} bandwidth the VX1751 will cause 70\% attenuation in signals of \SI{0.7}{ns} risetime. Since the signals studied here have rise time on the order of 5-\SI{15}{ns} Bandwidth should not be a limitation.

%Slew rate

The digitizer is controlled WaveDump, version 3.8.1, published by CAEN under the terms of the GNU General Public License\cite{WaveDump}. It uses the proprietary digitizer control libraries, also published by CAEN\footnote{The version used in this work was modified to write all data to a single file rather than one file per channel.}. 
WaveDump configures the digitizer according to a text file supplied by the user. The number of data points per trigger is defined globally for all enabled channels. For each enabled channel, the signal polarity, trigger threshold and baseline offset need to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a pulse must have in order to be recorded. The baseline offset determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset.

In the data presented in this thesis, the NE213 detector was connected to channel 0 of the digitizer and a YAP detector was connected to channel 1. Both NE213 and YAP pulses were of negative polarity. An amplitude threshold of \SI{48.8}{mV} was applied to the NE213 detector and a threshold of \SI{9.8}{mV} was applied to the YAP detector. A baseline offset of 40\% was accidentally applied to the NE213 channelthe and an offset of 10\% was applied to  YAP channel. This meant that high amplitude NE213 pulses were clipped (the pulses saturated the dynamic range of the digitizer resulting in pulses with a flat top). The effect this had on the energy calibration will be discussed in chapter \ref{ch:results}.

For a one hour run with the NE213 detector placed 1.05 m from the source and one YAP connected the output file is a $\sim$\SI{120}{GB} text file. The Python library Dask was used for processing the data, because it is optimized for processing datasets that are two large to fit in random-access memory and because it runs on all available processor cores \cite{Dask}. After processing and data reduction, the data were saved as a parquet file of size 7.3 GB. The Python library Pandas was used for additional processing and visualization of the reduced data set \cite{Pandas}.

\subsubsection{Time stamping}
The digital setup needs a method for providing a time stamp for each pulse from the detectors. A global time stamp is provided by the digitizer for each acquired waveform (from hereon referred to as an event). However, each event is 1204 ns long, and the pulses do not begin at the exact same point in time within the acquisition window for each event. A software based CFD algorithm was implemented. The algorithm searched the first \SI{40}{ns} before the peak amplitude of the pulse for the first sampling point to rise 30\% above peak amplitude. Linear interpolation between this sampling point and the previous enabled a time stamp with resolution better than \SI{1}{ns} to be generated. In Figure \ref{fig:cfd_trig}, four pulses from the NE213 detector are plotted centered around their CFD trigger points. Although this time stamp is given with sub-ns precision the accuracy is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate, resolution and bandwidth. 

\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/goodevents.pdf}
        \caption[Examples of digitized pulses and their CFD trigger points]{Relative timing for NE213 pulses of different amplitude. A CFD algorithm is used to generate a precise timestamp. Here a CFD trigger level of 30\% of the maximum amplitude is used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsubsection{Data Selection}
As the digitizer only enforces a threshold, additional selection criteria need to be applied offline in order to filter the data set. Since The VX1751 triggers on all enabled channels simultaneously, it is advisable to discard all empty acquisition windows. This was done by first determining and subtracting a baseline. The baseline was determined by averaging over the first \SI{20}{ns} of each pulse. After that the peak amplitude relative to this new baseline was determined. A threshold was then enforced, by removing all pulses of amplitude below the threshold.

During the digitizer configuration, a too high baseline offset was applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. $\sim$6\% of the events, whose amplitude could not be contained in the available range, had their peaks clipped. These events were not discarded, but did have some effect on the QDC spectrum. The pulse height spectrum shown in Fig. \ref{fig:cutoffevents} highlights the clipped events in green.
\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption[Digitized pulse height spectrum]{Digitized pulse height spectrum from the NE213 detector. Events reaching the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}

Certain events were removed because they caused the baseline determination, pulse integration or CFD algorithm to produce spurious results. For example in the case of $\sim$3\% of all events (not counting events below trigger threshold) the baseline determination was deemed unsteady, see Fig. \ref{fig:badevents} (a). Events were removed when the standard deviation over the baseline determination window was greater than 2 mV. A subset of the times this happens because a pulse is located inside the baseline determination window as shown in Fig. \ref{fig:badevents} (b). This can be recognized from the CFD trigger time point being located withon the first \SI{20}{ns} and happens 0.0016\% of the time.

Figure \ref{fig:badevents} (c) shows a situation where the CFD trigger occurs so late in the acquisition window that not enough samples follow it to carry out the \SI{500}{\nano\second} long gate integration. This occurs in 1.1\% of the events.

0.00066\% of the events are filtered out because a peak is immediately  preceded by a smaller peak. Since the CFD algorithm searches the \SI{40}{ns} immediately prior to the peak amplitude, this may cause the CFD algorithm to trigger inside the preceding smaller pulse. Examples of this are shown in Fig. \ref{fig:badevents} (d).

\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
\textbf{Description}                          & \textbf{Percentage of events} & \textbf{Discarded} \\ \hline
Unstable baseline                             & 2.69\%                        & Yes                \\ \hline
CFD trigger in baseline determination window  & 0.0016\%                      & Yes                \\ \hline
CFD trigger too late for longgate integration & 1.1\%                         & Yes                \\ \hline
CFD algorithm failed                          & 0.000066\%                    & Yes                \\ \hline
Pulses were clipped                           & 5.68                          & No                 \\ \hline
\end{tabular}
\caption[Problematic events.]{Problematic events. The listed events were all problematic in some sense. Wether they were discarded or not has been specified in the rightmost column.}
\end{table}

\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.87\textwidth]{DigitalSetup/badevents.pdf}
        \caption[Examples of rejected digitized events]{Examples of rejected digitized events. Regions of interest are highlighted with red boxes. (a) The baseline was deemed unsteady. (b) A pulse is located inside the baseline determination window(a subset of (a)). (c) Less than \SI{500}{ns} follow the CFD trigger point, so the longgate integration can not be carried out. (d) Closely adjacent pulses cause the CFD algorithm to fail.}
    \label{fig:badevents} 
\end{figure}
\newpage
\subsubsection{Energy Calibration}
The energy calibration of the digital setup was carried out in a similar manner to the analog setup, using the Knox method. The pulses were integrated digitally over the same gate lengths used in the analog setup, namely 60 and 500 ns starting 25 ns before the CFD trigger. A major difference compared to the analog setup lies in the baseline determination. For the analog setup the pedestal was needed in the energy calibration in order to account for and subtract any baseline offset. It acts as a global baseline subtraction. This is not necessary in the digital setup since the baseline is subtracted on an event by event basis during the initial data processing. 

Both the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges produced by the PuBe source as well as a \SI{1.33}{MeV} Compton edge from a $^{60}Co$ source were used for the calibration. the calibration points are listed in table \ref{tab:knox_d}. In Fig. \ref{fig:D_QDC} The \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges are marked in purple and orange respectively. The $^{60}Co$ source has Compton edges at \SI{1.17}{\MeV} and \SI{1.33}{\MeV}, but only one edge is visible. It is assumed that this is the \SI{1.33}{\MeV} edge and that the \SI{1.17}{\MeV} edge lies beneath it. As with the analog setup the uncertainty is the greatest for the \SI{2.23}{\MeV} Compton edge.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	channel (mV$\cdot$ns)            & 3791 & 7080  & 15481  \\
	\hline
	E(MeV)          & 1.33    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(MeV_{ee})$ & 0    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
   	\captionsetup{width=0.435\linewidth}
	\caption[Table of values used for energy calibration, digital setup.]{Table of digitizer pulse integration channels fitted to known Compton edges. See text for details.}
	\label{tab:knox_d}
\end{table}

Unfortunately the baseline shift on the NE213 channel offset was set too high, at 40\%, reducing the range available to negative pulses to only 0.6 V. This particularly affects the 4.44 \si{\MeV} Compton edge. In spite of this problem Fig. \ref{fig:D_QDC} shows that the calibration points still follow a linear trend. The fit parameters are used to produce the calibrated x-axis in the upper panel.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.8\textwidth]{DigitalResults/Ecall.pdf}
        \caption[Energy calibration of the digital setup]{The \SI{1.33}{\MeV} Compton edge of the $^{60}Co$ source and the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges of the PuBe source have here been used to perform an energy calibration. Due to the event by event based baseline determination it is assumed that bin 0 corresponds to \SI{0}{\MeV}$_\text{ee}$}
    \label{fig:D_QDC}
\end{figure}

\subsubsection{Charge Comparisson Pulse Shape Discrimination}
Two approaches to PSD were explored with the digitizer. First the charge comparison method was implemented by integrating the waveforms over 60 and \SI{500}{nanoseconds} respectively, starting 25 nanoseconds before the CFD trigger. The same gate lengths employed by the analog setup were used here in order to make a direct comparison to the analog setup possible the same gate lengths were used. In addition to this the separation between neutron and gamma-ray distributions was linearized by fine tuning the parameters $a$ and $b$ as shown in Fig \ref{fig:psd_sketch}.

\subsubsection{Covolutional Neural-Networks}\label{sec:cnn}
The biggest advantage a digitizer offers is that it records the entire pulse rather than just extracting a few parameters from them. With the entire waveform available, PSD can be approached in ways that are not feasible with analog electronics. One such approach is to apply an artificial neural network to the task of discriminating neutrons from gamma-rays. 
Artificial neural networks are function approximators, which use learned weights to perform a specified task. There are a variety of different network types, but for image classification tasks such as PSD, Convolutional neural networks set the standard. 

Training neural networks to perform neutron/gamma-ray PSD is not a new approach. It has been successfully implemented in a number of studies for various scintillators. See for example \cite{Griffiths}. The key difference to what has previously been done and what is done here lies in the way the training data is selected.

When constructing a neural network one needs a set of labelled data to train on\footnote{This is the case for supervised learning. There are also ways to train on unlabbeled data, i.e. unsupervised learning, but these have not been explored in this thesis.}. In order to train a network to discriminate between neutron and gamma-rays one needs a set of patterns representing the pulses and a label for each pattern defining its species. One approach would be to create a simulation and train the network on the generated data. Then the labels can be known with certainty to represent the patterns they are assigned to. On the other hand the simulated data will need to be a good representation of actual detector signals. This also implies that new simulations are needed for new detectors. Another approach is to use different pulse shape discrimination techniques for labelling data as either neutrons or gamma-rays. This is the approach taken by  Griffiths et al.\cite{Griffiths}. They label their training data by plotting the number of samples in a given pulse that surpasses a certain threshold as a function of peak amplitude, and then making a cut to separate neutrons from gamma-rays. This approach is effective as long as the model used to generate the training data is not systematically mislabeling a certain type of pulses (for example in a certain energy range). 

The approach taken here has been to take advantage of the extra information given by the time of flight spectrum. From the time of flight spectrum one immediately has access to a labeled set of neutron and gamma pulses in the form of the neutron bump and the gamma flash, see fig \ref{fig:CNN_data}. The downside of this approach is that the time of flight spectrum will also contain random coincidences, which means that the training data will contain some neutrons mistakenly labeled as gamma-rays and vice versa. Since the background is made up of random coincidences and makes up a small amount of the total training data, it is expcted that the false neutrons/gamma-rays wont cause the network to systematically misclassify, but merely slow down the training.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_data.pdf}
        \caption[Selection of training and validation data]{Selection of training and validation data using ToF information. Gamma flash events are labelled 0 and events from the neutron bump are labelled 1.}
    \label{fig:CNN_data} 
\end{figure}


Convolutional neural network use kernels of weights to extract features from an input image. The essential features of a kernel is the size, the stride and the weights. Figure \ref{fig:CNN_frame1} shows an example of a kernel being applied to an input vector $\vec{\textrm{x}}$. When applying the netwrok to PSD, then $\vec{\textrm{x}}$ will be a digitized current pulse from the NE213 detector. Here the convolution is performed with stride 2, i.e. the kernel is moved across the input in steps of 2, and the kernel has size 3. The elements of the kernel are called it's weights. Theses are the parameters which will be optimized through training. The kernel is scanned across $\vec{\textrm{x}}$ and a new vector $\vec{\textrm{h}}$ is produced. Each element of $\vec{\textrm{h}}$ is provided by a function, $\phi$, which takes the dot product of the kernel and a section of $\vec{\textrm{x}}$ as input. A common choice of activation function for CNNs is the rectified linear unit, ReLU given by:
\begin{equation}
	ReLU(x) = max(0,\;x)
\end{equation}

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_frame1.pdf}
        \caption[Convolution of a vector and a kernel, producing a new vector or feature map.]{Convolution of a vector and a kernel, using an activation function $\phi$.}
    \label{fig:CNN_frame1} 
\end{figure}

Convolutional neural networks typically employ multiple layers each containing multiple filters. The kernels in the first layer will produce new vectors or \textit{feature maps} $h_1$, $h_2$ and $h_3$ which highlight simple features in the data, see fig \ref{fig:CNN_frame2}. The kernels in the following layer will be scanned across these simple feature maps, extracting more complex features. Notice that in the second convolutional layer in figure \ref{fig:CNN_frame2} each of the 4 kernels operate on all three feature maps from the preceeding layer, producing 4 new feature maps $g_1$, $g_2$, $g_3$, $g_4$. Following the second convolutional layer the 4 feature maps are flattened into a single vector, $f$. The output of the network is given by a function $\phi_{out}$, which takes a linear combination of $f$ as input. A common choice of activation function for $\phi_{out}$ is the logistic function, which is bounded between 0 and 1:
\begin{equation}
	f(x) = \frac{1}{1-e^{-x}}
\end{equation}


\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_frame2.pdf}
        \caption[Two-layer convolutional neural network.]{Two-layer convolutional neural network. Two convolutional layers applied in sequence. Notice that kernels in the second }
    \label{fig:CNN_frame2} 
\end{figure}

A CNN was implemented using Keras 2.2.4\footnote{Keras is a high level API for constructing deep learning models. It can be run uses various backends for carrying out operations on tensors. Here Tensorflow 1.12.0 was used as backend.}\cite{keras}. The main features of the chosen architecture are summarized in table \ref{tab:architecture}. In addition to convolutional layers this network also contains max pooling layers. Maxpooling layers reduce the size of the feature maps by passing on only the maximum value of a given neighbourhood of the input vector. With a size of 2 and stride of 2 input vectors are reduced to half their size. Due to the kernel size and stride as well as the maxpooling each node in the flattened layer is indirectly connected to a large number of samples in the input layer. Both convolutional layers employ the ReLU activation function (max(0, x)), while the final layer applies the logistic function.
\begin{table}[h]
\center
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{\textbf{Input layer}: vector size 300}                                            \\ \hline
\textbf{Hidden layers}       & Kernel size & N kernels & Stride & Activation fcn \\ \hline
\textbf{Convolutional layer} & 9x1           & 10                & 4             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\textbf{Convolutional layer} & 5x10           & 16                & 2             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\multicolumn{5}{|c|}{\textbf{Fully connected} size 108}
\\ \hline
\multicolumn{5}{|c|}{\textbf{Output layer} size 1, \textbf{activation function}: Sigmoid}               \\ \hline
\end{tabular}
\caption{Table of the essential parameters in the CNN.}
\label{tab:architecture}
\end{table}



\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.8\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption[Training and validation accuracy of the CNN]{The validation and training accuracy plotted as a function of training iteration. both curves have been smoothened using an averaging filter in order to make trends more apparent.}
    \label{fig:CNN_training} 
\end{figure}

The network was trained on events from the gamma and neutron peaks of a 75 minute data set. 300 samples (or equivalently nanoseconds) from each pulse were used starting \SI{20}{ns} prior to the CFD trigger point. 75\% of the pulses or 3018 from the neutron bump and 3018 from the gamma flash were used as labeled training data while the remaining 25\% were used to evaluate the model as shown in Fig. \ref{fig:CNN_training}. Here the accuracy of the model is plotted as a function of training iteration (the number of times it has trained on the entire training data set). The blue curve shows the fraction of correctly labeled pulses achieved on the training data as a function of iteration, while the red curve shows the same thing on the validation data. The green and orange curves show the same data but after being smeared by a 9 channel averaging filter in order to make trends clearer. Since both training and validation data contains some fraction of incorrectly labeled background events the model is not expected to reach 100\% accuracy on either data set. In fact in this case that would be a sign of overfitting. This plot shows that although the performance on the training set keeps increasing, the performance on the validation set quickly levels out. For this reason the model achieved at iteration 53 is used. The network is still learning beyond iteration 53, but it is no longer learning features that generalizes to the validation data. Rather it is overfitting to noise in the training data.








\end{document}