\documentclass[main.tex]{subfiles}
\begin{document}

\section{Methods}\label{sec:exp_setup}
\subsection{Experimental Setup}
All data presented in this thesis was produced at the Source Testing Facility at the Department of Physics, Lund University, using two different DAQ systems. The first setup uses various NIM and VME modules to generate triggers for TDC and QDC modules. The output of the TDC and QDC modules is sent to a computer, where it is used to generate time of flight and QDC spectra in real time. Since this setup does most of the data processing via analog electronics it setup will be referred to as the \textit{analog setup}. 

The second setup is based on a digitizer which records detector signals as digital waveforms, for offline analysis. Since all of the processing of signals here happens digitally this setup will be referred to as the \textit{digital setup}. 
Both DAQS share the same physical setup of detectors, shielding and radiation source. By sending the detector signals through an active splitter both DAQS can even be run in parallel.


\begin{figure}[ht]
	\begin{subfigure}[b]{0.5\textwidth}
	    \centering
			\captionsetup{width=.80\linewidth}	
    	    \includegraphics[width=\textwidth, angle =90]{AnalogSetup/aquadaq.jpg}
        	\caption{Photo of the rack containing the analog setups electronics, as well as the powersupply (blue box) powering the detectors.}
	    \label{fig:aquadaq_image} 
	\end{subfigure}
	\begin{subfigure}[b]{0.24\textwidth}
	    \centering
	    	\captionsetup{width=1\linewidth}	
    	    \includegraphics[width=\textwidth, angle=180]{AnalogSetup/aquadaq_zoom.jpg}
        	\caption{Closeup of part of the analog setup.}
	    \label{fig:aquadaq_zoom_image} 
	\end{subfigure}
	\begin{subfigure}[b]{0.24\textwidth}
    	\centering
			\captionsetup{width=.6\linewidth}	
        	\includegraphics[width=\textwidth, angle =90]{DigitalSetup/digitizer.jpg}
        	\caption{The 8 channel VX1751 digitizer.}
    	\label{fig:digitizer_image} 
    \end{subfigure}
\end{figure}

\subsubsection{Radiation Source}
Two radiation sources were used in this work $^\text{239}\text{Pu}^\text{9}\text{Be}$ and $^\text{60}\text{Co}$, for tagging fast neutrons and energy calibration respectively. The fast neutron source is based on the alpha berylium reaction described in the previous chapter.
$$\text{\textalpha}+^\text{9}\text{Be}\rightarrow^\text{12}\text{C+n}$$
Via this reaction the fast neutron source produces approximately $\text{2.99}\cdot\text{10}^\text{6}$ neutrons per second\cite{Scherzinger:2017}. Often the carbon nucleus will be left in its first excited state which deexcites via emission of a \SI{4.438}{\mega\eV} gamma ray. This is the case for 58\% of the reactions\cite{Scherzinger:2015}\footnote{This has been established using an $^\text{241}$Am/$^\text{9}$Be source, but since $^\text{241}$Am produces alpha particles in an energy range similar to $^\text{239}$Pu it can be expected to hold for the PuBe source as well. The weighted mean energies of the alpha particles produces in the decay of $^\text{241}$Am and $^\text{241}$Pu respectively are: \SI{5.4786}{\mega\eV} MeV and \SI{5.4891}{\mega\eV}\cite{Scherzinger:2017}}.

$$^{12}C^ {*}\rightarrow^{12}C+\gamma_2$$
We then have the gamma produced in the decay of the plutonium nucleus, the neutron produced by the alpha particle hitting the Berylium nucleus and the gamma ray produced in the decay of the carbon nucleus, all of which happens within a very short time frame. 

The neutrons will come at a continuous range of energies depending on the Q value of the reaction:
$$Q = (m_\alpha + K_\alpha + m_{Be}) - (m_{C} + m_n)$$
The Q value thus depends on the initial energy of the alpha particle and whether the carbon nucleus is in its ground state or first or second excited state. The time of flight measurements described in the previous chapter then concists in detecting either the two gamma rays or the neutron and one of the gamma rays.

The $^\text{60}\text{Co}$ decays to $^\text{60}\text{Ni}$ via beta decay. This leaves the Nickel nucleus in an excited state which then deexcites, releasing gamma reays of 1.17 and 1.33 MeV gamma rays\cite{Nudat}.

\subsubsection{Fast Neutron and Gamma Ray Detectors}
In this work fast neutrons and gamma rays from the sources have been detected with an NE213 detector. This detector consists of a 15x15x15 \si{\cm}$^\text{3}$ volume of the organic liquid scintillator NE213, connected to a photomultiplier tube via a lightguide. This detector was constructed by Johan Sj√∂green as part of his thesis work 2009-2010\cite{sjogren}.

PICTURE

NE213 was previously produced by Nuclear Enterprises, but is equivalent to EJ301, currently produced by Eljen Technology. 

NE213 has excellent pulse shape discrimination capabilities, but is also highly toxic and has a low flash point (26$\text{\degree}$). At the STF it is used both for tagging fast neutrons and for discriminating between neutrons and gamma rays. 

Near the source four YAP detectors are placed. Yap detectors are composed of a Cerium doped Yttrium Aluminum Perovskite crystal mounted on a photomultiplier tube. As the YAP detectors are located close to the neutron source approximately \si{10\cm} they experience a larger flux than the NE213 detector located more than a meter away. This makes time resolution and decay time critical factors in their performance. The metastable states of the YAP pulses have decay times of of around 27 ns, but individual high amplitude pulses may be several hundred nanoseconds long, which means that the detector can handle count rates in the low MHz range.

\subsubsection{The Aquarium}
The fast neutron source was located inside a tank of water, referred to as the Aquarium, of dimension 140x140x140 \si{\cm}${}^\text{3}$, shown in figure \ref{fig:aquarium}. By moderating fast neutrons as well as absorbing some of them via the reaction: $^{\text{1}}$H(n,\textgamma)$^{\text{2}}$D$^*$, the water both provides shielding from the radiation and gives rise to a distinguishable gamma energy peak of 2.23 MeV, produced in the deexcitation of the deuteron. This peak can also be used in the energy calibration.

The aquarium has four experimental ports, intesecting at the central volume, where the source and yap detectors are placed. The cylinders and the central volume are airfilled, since they are not sealed off from the outside in any way. However, each of the ports can be closed with plastic plugs, to block out neutrons.

The PuBe source is located on the central vertical axis and can be in raised position at the same height as the ports or in a lowered parked position, where there is no direct line of flight through air from source and out of the ports.

The four YAP detectors are located near the source, but slightly higher, to allow a direct line of flight from source to any neutron detectors on the other side of the ports. The one used in this work is marked with an arrow.
\begin{figure}[ht]
	\center
	\begin{subfigure}[b]{0.49\textwidth}
	    \centering
    	    \includegraphics[width=\textwidth]{AnalogSetup/aquarium.pdf}
        	\caption{}
	    \label{fig:aquarium_pers} 
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
	    \centering
    	    \includegraphics[width=\textwidth]{AnalogSetup/aquariumbirdview.pdf}
        	\caption{}
	    \label{fig:aquarium_bird} 
	\end{subfigure}
	\caption{(a) Image of the aquarium and the NE213 detector located in front of one of the ports. The source and the four YAP detectors are located in the tubes at the center of the aquarium. (b) Top view of the aquarium and the detectors.}
	\label{fig:aquarium}
\end{figure}

\subsection{Analog Signal Processing}
The detector signals are copied by a FIFO module and copies are sent on to the analog and the digital DAQ setups as shown shown in figure \ref{fig:setup}. The FIFO module used, CAEN N625 has a max output voltage of 1.6 V, which is sufficient to copy detector signals but means some cosmic muons will be clipped.  On the analog side the signals are processed using a number of VME modules before finally time and energy information is sent through optical link to a computer where it is written to hard disc. 

\begin{figure}[h]
    \centering
        \includegraphics[width=1\textwidth]{AnalogSetup/setup_flowchart.pdf}
        \caption{Schematic diagram of the experimental setup consisting of source and detectors and the two DAQ setups.}
    \label{fig:setup}
\end{figure}

When the pulses first arrive at the analog setup they are sent through a threshold discriminator. This device allows pulses above a certain amplitude to pass and blocks pulses below the threshold from passing. For the NE213 the threshold was set at 94.6 mV and for the YAP it was set at 25 mV. The purpose of this discriminator is to filter out noise and low energy background gammas, so that these signals don't occupy the DAQ unnecessarily.

After the threshold discriminator the YAP and NE213 pulses are copied once again by a FIFO module. One copy will be used for charge integration while another is used to produce timestamps for time of flight determination. The timestamps are acquired with a CFD. The motivation for using this device is that simply triggering on the leading edge will lead to time walk for similarly shaped pulses of varying amplitudes. By instead triggering on the point where a pulse reaches a certain fraction of its peak amplitude time walk can be nearly eliminated\citep[pg.327]{Leo}.

In practice this is achieved by copying the pulse, inverting and attenuating one copy and delaying the other one. The pulses are then added and the zero crossing of the summed signal corresponds to a certain fraction of the input pulse. Figure ... demonstrates that the trigger point is amplitude independent. The output of the constant fraction discriminator is a logic pulse. representing the time of the CFD triggering.

Figure \ref{fig:setup} shows that the NE213 logic pulses from the CFD are sent to a trigger. The trigger then closes a latch, blocking additional signals from passing and sends logic pulses to the TDC and QDC modules. The logic pulse send to the TDC acts as a start signal and the latch remains closed until a stop signal has been received. The stop signal is provided by YAP signals, which have been delayed in order to reach the TDC after the NE213. Intuitively it might seem more straightforward to simply use YAP signals as start and NE213 as stop signal, without bothering with delaying signals. The reason this approach is not taken is that you will get a higher fraction true time coincidences if you spent the systems livetime triggering on the detector covering the smallest solid angle (ignoring differences in detector efficiency), since the propability of the detected particles partner hitting the other detector is maximized this way. This point can be expressed in terms of Bayes theorem in the following manner:
$$P(NE213|YAP) = P(YAP|NE213) \frac{P(NE213)}{P(YAP)} < P(YAP|NE213)$$ 

The trigger also generates a longgate (500 ns) and a shortgate (60 ns) logic pulses which are sent to the QDC modules. The QDC module then integrates the analog NE213 pulses for the duration of the logic pulses. This provides a measure of the energy deposited in the detector by the pulse on two different timescales. As shown in figure \ref{sec:setup} the QDC and TDC sends their output via optical link to a computer, where it is written to the harddrive.

\subsection{Time calibration of the analog setup}
The time of flight values provided by the analog setup are given in units of TDC channels, so a calibration was necessary. By using a copy of the yap signal to trigger the start of the TDC and another copy to trigger the stop a very sharp peak was produced in the TDC spectrum.
This peak was then shifted by delaying the signal by known numbers of nanoseconds and the calibration fit shown in figure \ref{fig:Tcal} was generated. Since the location of $t=0$ is arbitrary the important result is the coefficient \SI{0.28}{\nano\second/channel}.
\begin{figure}[ht]
	\centering
    	\includegraphics[width=0.8\textwidth]{AnalogSetup/Tcal.pdf}
        \caption{Top: Histogram of the calibration TDC spectrum. Each peak represents a different delay value for the stop pulse. Bottom: A linear fit applied to the TDC and delay values.}
	    \label{fig:Tcal} 
\end{figure}

\subsection{Digital signal processing}
\subsubsection{Digitizer Specifications and Configuration}
A digitizer is a device which converts analog signals, of amplitude greater than a chosen threshold, to digital waveforms. This means that if a signal has high enough amplitude, the digitizer triggers and the continuous analog signal is approximated by a list of numbers, each representing a single sampling point, along with some additional information such as timestamp and channel number. By this definition a digitizer is not much different from a digital oscilloscope. The main difference being that the oscilloscope has a display and is optimized to be easy to move around and use for realtime diagnostics, while the digitizer is optimized for efficient high rate data transmission to a computer where further analysis can be carried out, either offline or online.

The advantage of a digitizer over a traditional analog data acquisition system is that it allows the user to process a single data set in multiple ways in order to find ideal values for parameters, without having to change anything in the physical setup or acquiring new data. On the downside discretizing a continuous signal means that you are limited by the sampling rate, resolution and bandwidth of the digitizer as well as the data rate of your read-out system.

The digitizer used in this work is an 8 channel CAEN VX1751 digital waveform digitizer. It has a sampling rate of \si{1\giga\sample\per\second} in standard mode, but can also be operated in Double edge sampling mode, which disables 4 cannels but increases the sampling rate to \si{2\giga\sample\per\second}\cite[p.9]{CAEN}. The data presented in this thesis was acquired in standard mode. This results in one data point per nanosecond when digitizing signals. Given a set of samples there will always be an infinite number of waveforms that fit the points\cite{Spectrum}. This is called aliasing. However, if the sampling rate is at least twice as large as the highest frequency component of your signal then aliasing is avoided. This is known as the Nyquist Theorem. Typically though a higher sampling rate is desirable.

In order to reproduce signals with a high fidelity it is not enough to have a high sampling rate. The dynamic range, resolution and bandwidth also need to be considered. The VX1751 digitizer has a \si{1\volt} dynamic range, which means that the difference between maximum and minimum voltage is \si{1\volt}. How this range is used is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bit, so the \si{1\volt} dynamic range is cut in 1024 pieces each of size 0.978\si{\milli\volt}\cite{CAEN}. 

The VX1751 has a bandwidth of \si{500 \mega\hertz}. The bandwidth is important because it determines the frequency range of signals that can be read without significant attenuation. If the frequency of a signal is too high, then the amplitude recorded by the digitizer will be lower than the actual signals amplitude. The Bandwidth, $B$, is related to the rise time (the time it takes the pulse to rise from 10 to 90\% of peak amplitude), $T_{rise}$, through the following formula\cite[p.354]{Leo}
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{rise}
\end{equation}
Equation \ref{eq:bandwidth} tells us that an analog signal with a 1 ns rise time would need a 350 MHz bandwidth digitizer in order for the digitized signal to only be attenuated to 70\% of the analog signals amplitude. In comparison the typical rise time of the signals studied in this work is around 5-15 ns.

The digitizer is controlled via CAEN's software WaveDump, published under the terms of the GNU General Public License\cite{WaveDump}, which uses CAEN's proprietary digitizer control libraries\footnote{WaveDump was modified to write all data to a single file rather than one file per channel.}. WaveDump configures the digitizer according to a text file supplied by the user. The number of data points per trigger is defined globally for all enabled channels. For each enabled channel the signal polarity, trigger threshold and baseline offset needs to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a signal must have in order for an event to be recorded. 

The baseline offset is given in percent and determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset.

In the data presented in this thesis the NE213 detector was connected to channel 0 and a YAP detector connected to channel 1, and assigned negative polarity. A threshold of 48.8 mV (50 ADC) was applied to the NE213 and a threshold of 9.8 mV (10 ADC) was applied to the YAP channel. A baseline offset of 10\% was applied to the YAP channel and an offset of 40\% was accidentally applied to the NE213 channel. This meant that high amplitude pulses were cut, which as will be shown in a later chapter affected the energy calibration.

For a one hour run with the NE213 detector placed 1.05 m from the source and one YAP connected the output file would be approximately 120 GB. The Python library Dask was used for processing the this file, because it is optimized for processing larger than memory data sets in chunks and because it runs on all available processors by default\cite{Dask}. The processed data was saved as a parquet file of size 7.3 GB. The Python library Pandas was used for additional processing and visualization of the reduced data set\cite{Pandas}.

\subsubsection{Timestamping}
Just as with the analog setup the digital setup needs a method for providing a timestamp in order to produce a precise timestamp for each pulse from the detectors. A general timestamp is provided by the digitizer for each event, but each event is 1204 ns long, and the pulses do not begin at the exact same time in the acquisition window for each event. For this reason a software based constant fraction discriminator was implemented. This algorithm looks at the first 40 ns before the peak amplitude and finds the first sampling point to rise above  a certain fraction of the peak amplitude (In this work 30\% was found to work well). Using linear interpolation between this point and the previous point a sub nanosecond timestamp is generated. In figure \ref{fig:cfd_trig} four pulses from the NE213 detector have been plotted centered around their CFD trigger points. Although this timestamp is given with sub nanosecond precision the accuracy is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate, resolution and bandwidth. 

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/goodevents.pdf}
        \caption{A CFD algorithm is used to generate a precise timestamp. Here a CFD trigger level of 30\% of maximum is used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsubsection{Data Processing}
Since the VX1751 triggers on all enabled channels simultaneously it is necessary to throw away all empty acquisition windows. This is done by first determining and subtracting a baseline by averaging over the first \si{20\nano\second} of each pulse. After that the peak amplitude relative to the new baseline is found and a chosen threshold is reinforced, by removing all pulses of amplitude below the threshold.

Certain events are removed because they cause the baseline determination, pulse integration or CFD algorithm to produce unreliable results. For example in the case of 2.69\% of all events (not counting events below trigger threshold) the baseline determination is deemed unreliable because the baseline is too unsteady. 
Examples of this are shown in figure \ref{fig:badevents} (A). The threshold for this cut was chosen to be when the standard deviation over the baseline determination window is above 2 mV. 
A subset of the times the baseline determination fails will be due to a pulse rising inside the baseline determination window as shown in figure \ref{fig:badevents} (B). This can be recognized from the location of the CFD trigger and happens 0.0016\% of the time. Events are also removed if they occur too late in the acquisition window. Figure \ref{fig:badevents} (C) shows a situation where the CFD trigger occurs so late that there is not enough samples following it to carry out the \si{500 \nano\second} long gate integration, which will have an adverse effect on the QDC spectrum and pulse shape discrimination. This occurs in 1.1\% of the events.

0.00066\% of the events are filtered out because a peak is preceded by a smaller peak causing the CFD algorithm to trigger inside the smaller pulse. This situation results in the cfd algorithm finding a positive slope, rather than negative as one would expect for a pulse with negative polarity. Examples of this is shown in figure \ref{fig:badevents} (D).
\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/badevents.pdf}
        \caption{Since The NE213 detector is used for pulse shape discrimination it is important that (A) the the baseline is stable, (B) that the pulse rise does not land in the baseline determination window (a subset of (A)), (C) the tail of the pulse is contained. (D) Moreover Closely adjacent pulses may on rare occasions cause the CFD algorithm to find a slope of the wrong sign.}
    \label{fig:badevents} 
\end{figure}

During the digitizer configuration a too high baseline offset was applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. Consequently 5.67\% of the events were partly clipped. These events were not thrown away, but did have some effect on the energy spectrum.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption{Digitized pulse height spectrum from the NE213 detector. Events reaching the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}

\subsection{The Data Sets}
The data presented in the results chapter was collected during one hour of parallel measurements at the analog and the digital setup. The applied thresholds are shown in table \ref{tab:settings}. The YAP threshold was set at 9.8\si{\milli\volt} during acquisition, but having such a low threshold was found to decrease the signal to noise ratio in the time of flight spectrum, so a higher threshold of 24.4\si{\milli\volt} was applied.
\begin{table}[bh]
\begin{tabular}{|l|l|l|l|l|}
\hline
Setup   & YAP threshold(mV) & NE213 threshold(mV) & NE213 events ($\text{10}^\text{6}$) & Livetime \\ \hline
Analog  & 25.0              & 94.6                & 4.3      & 44\%             \\ \hline
Digital & 9.8/24.4	        & 48.8                & 2.2      & -             \\ \hline
\end{tabular}
\caption{Threshold values and number of NE213 events.}
\label{tab:settings}
\end{table}

\subsubsection{Charge Comparisson PSD}
Two approaches to pulse shape discrimination were explored with the digitizer. Firstly the charge comparison method was implemented by integrating the waveforms over 60 and 500 nanoseconds respectively, starting 25 nanoseconds before the CFD trigger. Other gate lengths were also attempted, and actually it seemed beneficial to keep both somewhat shorter. However, to make a direct comparison to the analog setup possible the same gate lengths were used. In addition to this the separation between neutron and gamma ray distributions was linearized by adding baseline offsets to the charge integrals.

\subsubsection{Neural Network Based Pulse Shape Analysis}\label{sec:cnn}
The biggest advantage a digitizer offers is that we record the entire pulses rather than just extracting a few parameters from them. With the entire digitized waveform available pulse shape discrimination can be approached in ways that are not feasible with analog electronics. Neural networks are known to be able to approximate any continuous function provided they receive sufficient data to train on and a sufficiently complex architecture. So with a labeled set of data it should be possible to train a network to discriminate neutrons from gammas. 

A digitized pulse can be seen as a 1-dimensional image of a pulse, where each element is a discrete approximation to the original analog signal. In recent years convolutional neural networks have set the standard in image classification tasks, so it makes sense to apply a CNN to the problem of pulse shape discrimination.  In contrast to a multilayer perceptron, where each node is connected to every node in the subsequent layer via weights, a CNN uses kernels, with trainable weights to generate images of smaller dimensions. This approach greatly decreases the number of weights necessary in the network, which speeds up both training and application\cite[p.330]{Goodfellow-et-al-2016}.
%insert images

The main features of the chosen architecture are summarized in table \ref{tab:architecture}. Due to the kernel size and stride as well as the maxpooling the receptive field of each node in the flattened layer is connected to 50-54 samples in the input layer. Both convolutional layers employ the ReLU activation function (max(0, x)), while the final layer applies the logistic function.
\begin{table}[hb]
\center
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{\textbf{Input layer}: vector size 300}                                            \\ \hline
\textbf{Hidden layers}       & Kernel size & N kernels & Stride & Activation fcn \\ \hline
\textbf{Convolutional layer} & 9           & 12                & 4             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\textbf{Convolutional layer} & 5           & 12                & 2             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\multicolumn{5}{|c|}{\textbf{Fully connected} size 108}
\\ \hline
\multicolumn{5}{|c|}{\textbf{Output layer} size 1, \textbf{activation function}: Sigmoid}               \\ \hline
\end{tabular}
\caption{Table of the essential parameters in the network.}
\label{tab:architecture}
\end{table}

Training of neural networks to perform neutron - gamma ray pulse shape discrimination is not a new approach. It has been successfully implemented in a number of studies for various scintillators. The key difference to what has previously been done and what is done here lies in the way the training data is selected.

When constructing a neural network one needs a set of labelled data to train on\footnote{This is the case for supervised learning. There are also ways to train on unlabbeled data, i.e. unsupervised learning, but these have not been explored in this thesis.}. In order to train a network to discriminate between neutron and gamma rays one needs a set of patterns representing the pulses and a label for each pattern defining its species. One approach would be to create a simulation and train the network on the generated data. Then the labels can be known with certainty to represent the patterns they are assigned to. On the other hand the simulated data will need to be a good representation of actual detector signals. This also implies that new simulations are needed for new detectors. Another approach is to use different pulse shape discrimination techniques for labelling data as either neutrons or gamma rays. This is the approach taken by  Griffiths et al.\cite{Griffiths}. They label their training data by plotting the number of samples in a given pulse that surpasses a certain threshold as a function of peak amplitude, and then making a cut. This approach is effective as long as the model you use to generate your training data from is not systematically mislabelling a certain type of pulses (for example in a certain energy range). 

The approach taken here has been to take advantage of the extra information given by the time of flight spectrum. From the time of flight spectrum one immediately has access to a labeled set of neutron and gamma pulses in the form of the neutron and gamma peaks. The downside of this approach is that the time of flight spectrum will also contain random coincidences, which means that the training data will contain neutrons mistakenly labeled as gamma rays and vice versa. Since the background is made up of random coincidences and makes up a small amount of the total training data, the false neutrons/gamma rays wont cause the network to systematically misclassify, but rather make training take longer.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption{The validation and training accuracy plotted as a function of training iteration. both curves have been smoothened using an averaging filter in order to make trends more visible.}
    \label{fig:CNN_training} 
\end{figure}

The network was trained on events from the gamma and neutron peak of a 75 minute data set. 300 samples (or equivalently nanoseconds) from each pulse were used starting 20 ns prior to the CFD trigger point. 75\% of the pulses were used as labeled training data while the remaining 25\% were used to evalute the model as shown in figure \ref{fig:CNN_training}. Here the accuracy of the model is plotted as a function of training iteration (the number of times it has trained on the entire training dataset). The blue curve shows the performance achieved on the training data while the red curve shows the performance on the validation data. The green and orange curves show the same data but after being smeared by a 9 channel averaging filter in order to make trends clearer. This plot shows that although the performance on the training set keeps increasing, the performance on the validation set quickly levels out. For this reason the model achieved at iteration 28 is used. The network is still learning beyond iteration 28, but it is no longer learning features that generalizes to the validation data. Rather it is overfitting to noise in the training data. After training the model was tested on a different data set taken in a ten minute run. Seeing it performed well on this data the model was chosen to be applied on the final data set used in this thesis.


\subsection{Energy Calibration}
Each particle interacting in the detector will result in a pulse of charge. In the analog setup charge-to-digital converters are used to parameterize these pulses into measures of the deposited energy. Pulses are integrated on two different timescales: 60 and 500 nanoseconds. Together they can be used to express the pulse shape, PS, through the fraction of charge located in the tail.

In the NE213 detector neutrons interact via the strong force with nuclei while gamma rays primarily interact electromagnetically with electrons. This means that the pulses they produce in the detector originate from entirely different particles and processes, and consequently the specrtrum can not be calibrated in terms of the particles kinetic energy. Instead it is customary to calibrate it in terms of the electron equivalent energy. That is the energy an electron would need to have had in order to produce the same integrated charge in the detector. One way of performing the energy callibration is through the Knox method, which is described in Bj√∂rn Nilssons PhD thesis\cite[pg. 55]{Nilsson}.

Because the QDC baseline has been known to occasionally drift by tens of channels during acquisition it was necessary to to observe and compensate for any drift. By allowing the acquisition start to trigger on YAP signals as well as NE213 signals a large amount of windows with no signals were integrated. These events represent the baseline offset. The number of YAP signals sent to the trigger was reduced by a factor of 256 with a prescaler. However, during this particular data acquisition the pedestal remained stationary.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	QDC(channel)             & 67.5 & 11.43 & 2718 \\
	\hline
	E(MeV)          & 0    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(MeV_{ee})$ & 0    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
   	\captionsetup{width=0.435\linewidth}
	\caption{Table of ADC values and the corresponding energies. See text for details.}
	\label{tab:knox}
\end{table}

The QDC spectrum is shown in figure \ref{fig:qdc_a} top panel. The upper x-axis shows the calibrated energy scale. The narrow peak located to the far left is the pedestal. It is produced when a YAP trigger gives a start signal causing the QDC module to integrate only the baseline. The bump immediately to the right of it is produced when this YAP trigger by chance coincides with something in the NE213 detector.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogResults/Ecall.pdf}
        \caption{Top: The raw QDC spectrum. Middle: The callibration fit produced with the Knox method. Bottom: The energy calibrated QDC spectrum.}
    \label{fig:qdc_a}
\end{figure}

The 2.23 and the 4.44 MeV Compton edges have been highlighted in green and red. Using the Knox method a set of points correlating the energy deposition in ADC channels to $MeV_{ee}$. The points are also shown in table \ref{tab:knox}. Using these points a linear fit was made and plotted in figure \ref{fig:qdc_a} bottom panel. With this fit the QDC spectrum was converted to $MeV_{ee}$. It can be seen that the uncertainty is greatest for the 2.23 MeV Compton edge.

By splitting the signals from the detectors it was possible to record data from the digital setup in parallel with the analog setup. A total of 2.2 million pulses were seen from the NE213 detector by the digitizer during one hour of measurements. The pulses were integrated digitally over the same gate lengths used in the analog setup, namely 60 and 500 ns starting 25 ns before the CFD trigger. A major difference compared to the analog setup lies in the baseline determination. For the analog setup we needed the pedestal in the energy calibration in order to account for and subtract any baseline offset. It acts as a global baseline subtraction. This is not necessary in the digital setup since the baseline is subtracted on an event by event basis during the initial data processing. 

The energy calibration was carried out using the 2.23 and 4.44 Compton edges produced by the PuBe source as well as a $^{60}Co$ source. As was the case for the analog setup the Knox method was used. In figure \ref{fig:D_QDC} The 2.23 and 4.44 $MeV_{ee}$ Compton edges are marked in purple and orange respectively. The $^{60}Co$ source has Compton edges at 1.17 and 1.33 $MeV_{ee}$, but only one edge is visible. It is assumed that this is the 1.33 $MeV_{ee}$ edge and that the 1.17 $MeV_{ee}$ edge lies beneath it. As with the analog setup the uncertainty is the greatest for the 2.23 MeV Compton edge.

Unfortunately the baseline shift on the NE213 channel offset was set too high, at 40\%, reducing the range available to negative pulses to only 0.6 V. This particularly affects the 4.44 $MeV_{ee}$ compton edge. It also cuts off the signals produced by cosmic rays, but these would still have been beyond the dynamic range even if the full 1 V had been available. Inspite of this problem figure \ref{fig:D_QDC} shows that the calibration points still follow a linear trend. The fit parameters are used to produce the calibrated x-axis in the upper panel.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.8\textwidth]{DigitalResults/Ecall.pdf}
        \caption{The 1.17 + 1.25 MeV edge of the $^60$ source and the 2.23 and 4.44 MeV Compton edges of the PuBe source have here been used to perform an energy calibration. Due to the event by event based baseline determination it is assumed that bin 0 corresponds to 0 $MeV_{ee}$}
    \label{fig:D_QDC}
\end{figure}





\end{document}