\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Method}
\section{Experimental Infrastructure}
\subsection{Source-Testing Facility}
The Source-Testing Facility (STF) at the Division of Nuclear Physics in Lund, Sweden, is a fully equipped user facility for the characterization of detectors, shielding and sources~\cite{Messi}.
The operation of the STF is a collaboration effort between the Division of Nuclear Physics and the ESS Detector Group. It is employed for the development of detectors for ESS and industry. The STF offers easy and reliable access to actinide/Be fast-neutron sources and gamma-ray sources. Additionally, the fast neutrons can be moderated to lower energies with various materials. The STF is divided into two areas, see Fig.~\ref{fig:STF}. The user area contains data-acquisition systems (DAQs), workstations as well as a wide range of electronics modules and detectors. The smaller interlocked area is where measurements are carried out. This area contains an array of shielding materials as well as a dedicated neutron-tagging setup based upon the Aquarium.

\begin{figure}[ht]
	\center
    	\includegraphics[width=0.7\textwidth]{AnalogSetup/STF.pdf}
	\caption[The Source-Testing Facility.]{The Source-Testing Facility. The smaller room to the left is the interlocked area, where measurements are carried out. The user area area to the right contains DAQ setups, workstations and storage. Figure from~\cite{Messi}.}
	\label{fig:STF}
\end{figure}

\subsection{The Aquarium}
The fast-neutron source may be located inside a 140$\times$140$\times$140 \si{\cm}${}^\text{3}$ tank of water referred to as the Aquarium, see Fig.~\ref{fig:aquarium}. The Aquarium has four horizontal cylindrical beam ports, intersecting at a central volume. A source and up to four gamma-ray detectors may be placed within the central volume.  The beam ports and the central volume are air filled, allowing neutrons and gamma-rays to reach a neutron/gamma-ray detector placed on the other side of the aquarium without passing through shielding. Each of the beam ports can be plugged when not in use.
By moderating and absorbing the fast neutrons the water tank both provides shielding from the sources and gives rise to a distinguishable gamma-ray energy of \SI{2.23}{MeV} produced in the de-excitation of the deuteron via the $^{\text{1}}$H($n$,$\gamma$)$^{\text{2}}$H$^*$ reaction. This gamma-ray is useful when performing energy calibrations, see Sec.~\ref{sec:Ecal_A} and Sec.~\ref{sec:Ecal_D}. 
An actinde/Be source may be positioned on the central vertical axis of the Aquarium and can be raised to the same height as the ports for ToF measurements. In a ``lowered" or ``parked" position, there is no direct line-of-sight through air from the source through the ports. The four dedicated gamma-ray detectors are located near the source, but are raised slightly to allow a direct line-of-sight from source through the ports, see Fig.~\ref{fig:aquarium}.
\begin{figure}[ht]
	\center
    	\includegraphics[width=\textwidth]{AnalogSetup/aquarium_combined.pdf}
	\caption[The Aquarium]{The Aquarium. Left panel: Oblique view with neutron and gamma-ray detector in front of one of the horizontal cylindrical beam ports. Lines-of-flight from source and out of two of the ports are indicated with arrows. The source and the four gamma-ray detectors are located in the tubes at the center of the aquarium. Right panel: Top view of the aquarium with source and detectors indicated by arrows.}
	\label{fig:aquarium}
\end{figure}




\subsection{Radiation Sources}
Two radiation sources were used in this work. The actinide/Beryllium fast neutron and gamma-ray source $^\text{238}\text{Pu}/^\text{9}\text{Be}$ (referred to as Pu/Be) was used for tagging fast neutrons and for energy calibration of the neutron/gamma-ray detector. As shown in Sec.~\ref{sec:freeNeutrons}, the PuBe source produces both a cascade of low-energy photons and fast neutrons, which 55\% of the time are accompanied by the emission of a \SI{4.44}{\MeV} gamma-ray. The source has been measured to produce approximately $\text{2.99}\cdot\text{10}^\text{6}$ neutrons per second~\cite{Scherzinger:2017}. The pure gamma-ray source $^\text{60}\text{Co}$ was used for detector calibration. $^\text{60}\text{Co}$ decays to excited states of $^\text{60}\text{Ni}$ via beta decay. De-excitation of $^\text{60}\text{Ni}$ will result in gamma-rays of energies 1.17 or \SI{1.33}{MeV}~\cite{Nudat}.



\subsection{Fast-Neutron and Gamma-Ray Detectors}
As ToF depends on the accurate timing of \textgamma /n and \textgamma /\textgamma\; pairs, it is essential to detect both particle species with accurate timing. In this work, two different detectors were used for detecting neutrons and gamma-rays from the sources. A liquid organic NE213 detector was used to detect both gamma-rays and neutrons while an inorganic Cerium doped Yttrium Aluminum Perovskite crystal (YAP) detector was used to detect gamma-rays. 

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/detectors.pdf}
        \caption[Photographs of detectors]{Photographs of detectors. Left: NE213 detector. Right: YAP detector.}
    \label{fig:detectors}
\end{figure}

Since its introduction in the early 1960s, the NE213 liquid organic scintillator has become the gold standard for fast-neutron detection due to its excellent neutron/gamma-ray discrimination capabilities and high detection efficiency. 
The drawbacks of this scintillator are that it is toxic and highly volatile with a flash point of 26$\text{\degree}$C. The NE213 used here was produced by Nuclear Enterprises. It is equivalent to EJ301, currently produced by Eljen Technology~\cite{Eljen}. The decay times of the first three scintillation components are \SI{3.16}{ns}, \SI{32.3}{ns} and \SI{270}{ns}~\cite{Eljen}. It is contained in a 122$\times$122$\times$179 \si{\mm}$^\text{3}$ volume which is connected to a photomultiplier tube via a lightguide\footnote{This detector was constructed by Johan Sj√∂gren as part of his thesis work in 2009-2010~\cite{sjogren}.}.

Near the source, four YAP detectors are placed. The YAP detectors are mounted on photomultiplier tubes. These inorganic scintillators are largely insensitive to neutrons and provide excellent timing of gamma-rays. As the YAP detectors are located closer to the source they experience a significantly higher gamma-ray flux than the NE213 detector does. This makes time resolution and decay time critical factors in their performance. The scintillation light has a decay time of $\sim$27 ns~\cite{Scionix}. Howevever, it takes $\sim$\SI{100}{ns} for signals to return to the original baseline level. This means that the detector can handle count rates in the low MHz range without significant pileup. To simplify the analysis and limit the data rates, only one YAP detector was used in this project.

\section{Signal Processing}
Two different experimental systems were employed. These experimental setups differed only in the DAQ system used and shared the same physical setup of detectors, shielding and radiation source. By sending the detector signals through an active splitter, both DAQs could even be run in parallel on the exact same detector signals. The first setup employed NIM modules to process signals and generate a trigger decision. VME modules were used to digitize the timing and charge characteristics of the signals. The digitized data were transferred to a computer, where it was saved and plotted in real time. Since this setup did most of the data processing via analog electronics, it will be referred to as the analog setup. The second setup was based on a digitizer which recorded detector signals as digital waveforms for offline analysis. Since all of the processing of the signals was performed digitally, this setup will be referred to as the digital setup. Figure~\ref{fig:DAQ} contrasts the analog and digital setups. The analog setup is composed of a variety of NIM and VME modules as well as a large number of LEMO cables. The digital setup is composed of a single digitizer, which accepted two raw detector signals as inputs and fits in a single VME slot. The digital setup is thus far more spatially compact.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/setup_flowchart.pdf}
        \caption[Schematic of the experimental setup]{Schematic of the experimental setup. Green box: the digital setup. Yellow box: the analog setup.}
    \label{fig:setup}
\end{figure}


\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/DAQ.pdf}
        \caption[Data-acquisition systems.]{Data-acquisition systems. The analog DAQ rack is shown to the left. Digitization modules are highlighted by the orange arrow, and the power supply is indicated by the blue arrow. A closeup of the analog electronics is indicated in the red box. The single digitizer module is shown highlighted in green to the right.}
    \label{fig:DAQ}
\end{figure}

\subsection{Analog}
The detector signals were replicated by a fan-in-fan-out (FIFO) module and copies were sent on to both the analog and the digital DAQ setups, see Fig.~\ref{fig:setup}. On the analog side, the signals were processed before time and energy information was sent through an optical link to a computer running Centos 7.3 where it was written to hard drive. 

\subsubsection{Discriminators}
The YAP and NE213 pulses were sent to constant-fraction discriminators (CFDs). The CFDs are labeled \circled{1} in Fig.~\ref{fig:setup}. The motivation for using CFDs was that ToF measurements required precision timing of the start and stop signals. Simply triggering on the leading edge of pulses will lead to ``time walk" for similarly shaped pulses of varying amplitudes. Time walk means that pulses of the same shape but different amplitude will result in triggers at different times, see Fig.~\ref{fig:discriminator}. As can be seen the smaller pulse passes the threshold at a later time than the larger pulse. By instead triggering on the point where a pulse reaches a certain fraction of its peak amplitude, time walk can nearly be eliminated~\cite{Leo}. This may be achieved by dividing or copying the signal, inverting one copy and delaying the other. The pulses are then added and the zero crossing of the summed signal corresponds to a fraction of the input pulse, see Fig.~\ref{fig:cfd}. Changing the amplitude of the incoming signal will have the same effect on both copies of the signal. Consequently the CFD trigger time, defined by the zero-crossing, is amplitude independent. The output of the CFD was a logic pulse representing the time of the CFD triggering. CFDs were also used to enforce a threshold of \SI{-94.6}{mV} for the NE213 detector and \SI{-25.0}{mV} for the YAP detector. Only pulses below the thresholds generated logic pulses.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.7\textwidth]{AnalogSetup/threshold.pdf}
        \caption[Time walk resulting from leading edge triggering]{Time walk resulting from leading-edge triggering. Figure adapted from Ref.~\cite{rofors}.}
    \label{fig:discriminator}
\end{figure}

\begin{figure}[h]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogSetup/cfd_emil.pdf}
        \caption[CFD trigger principle]{Illustration of the principle of an analog constant fraction discriminator. Figure from Ref.~\cite{rofors}.}
    \label{fig:cfd}
\end{figure}



\subsubsection{Trigger logic}
The NE213 logic pulses generated by the CFD were sent to a latch (labelled \circled{2} in Fig.~\ref{fig:setup}), which allowed this single pulse to pass and blocked any further pulses until it received a reset signal. The pulses that made it through were passed on to the data-acquisition modules presented below. The reset signal is given after data-acquisition modules have finished digitization ($\sim$\SI{10}{\micro s}) and the data has been written to a computer via optical link ($\sim$\SI{350}{\micro s}). The amount of time the latch was closed is called the deadtime. This is because the system is unable to process new events (effectively dead) during this time. The deadtime was measured by an uninhibited and an inhibited scaler connected to an oscillator. A scaler is a module which simply counts the number of logic pulses it receives. The oscillator incremented the scalers at a specific frequency. Whenever the latch was closed the inhibited scaler was no longer incremented. This made it possible to calculate the livetime as the ratio of inhibited scaler counts to uninhibited scaler counts every time the scalers were read. Note that although the setup contained both YAP and NE213 detectors, only the NE213 detector was used to trigger the acquisition. Since ToF measurements require both a start and stop signal, there can at most be as many coincidences as there are pulses in the detector with the lowest count rate. The NE213 experiences the lowest count rate as it subtends a smaller solid angle. Thus, the deadtime was minimized by triggering the acquisition on the NE213 detector.

\subsubsection{Time-to-digital converters}
A logic signal was also sent from the latch to a time-to-digital converter (TDC). This module is labeled \circled{3} in Fig.~\ref{fig:setup}. The TDC charges a capacitor with a constant current once a start signal is received, and stops once the stop signal is received. The start signal is the event logic pulse from the NE213 detector and the stop signal is a delayed event pulse from the YAP detector. The amplitude of the capacitor discharge pulse is proportional to the time between the start and stop~\cite{CAENTDC}. An analog-to-digital converter within the TDC digitizes the pulse and sends the resulting number to the computer via the optical link.
\begin{figure}[h]
	\centering
    	\includegraphics[width=0.8\textwidth]{AnalogSetup/Tcal.pdf}
        \caption[TDC calibration spectrum.]{TDC calibration spectrum. Top panel: Each peak represents a different delay value for the stop pulse. Bottom: A linear fit applied to the TDC and delay values.}
	    \label{fig:Tcal} 
\end{figure}

The raw values provided by the TDC had units of TDC channels, so a calibration was necessary to convert them into ns. By triggering the start of the TDC with a delayed version of the same signal used to stop the TDC, a very sharp peak was produced. This peak was then shifted over known intervals by increasing delay-cable length to produce the calibration data shown in Fig.~\ref{fig:Tcal}. Since the location of $t=0$ is arbitrary, the important result is the calibration coefficient \SI{0.28}{\nano\second/channel}, which was used to convert timing data from TDC channels to nanoseconds.

\subsubsection{Charge-to-digital converters}\label{sec:Ecal_A}
The pulses that made it through the latch were used to generate \SI{60}{\ns} and \SI{500}{\ns} logic pulses (with the first \SI{25}{\ns} preceding the CFD trigger point), which acted as integration windows or ``gates" for the charge-to-digital converters (QDCs). In Fig.~\ref{fig:setup}, these modules are labeled \circled{4}. Copies of the analog current pulses were also sent to the QDC modules. Each module carried out an integration for the duration of the applied gate. The resulting charges provided a measure of the energy deposited in the detector by a given pulse on two different timescales. The QDC modules sent their outputs via optical link to a computer, where they were written to the hard drive, see Fig.~\ref{fig:setup}.

The charge integration performed by the QDC modules is a measure of the energy deposited in a detector. In the NE213 detector, neutrons primarily interact via scattering from $^1$H while gamma-rays primarily interact with atomic electrons. It is customary to calibrate a fast neurtron detector with gamma-ray sources, employing the ``electron-equivalent energy". Electron equivalent energy corresponds to the amount of energy deposited by an electron. As gamma-rays interact with atomic electrons, they result in electron-equivalent deposited energies. One way of performing this energy calibration is the Knox method of examining the Compton edge corresponding to gamma-ray sources~\cite{Nilsson}. The maximum energy transferred by a gamma-ray to a recoil electron is given by:
\begin{equation}
	(E_{e})_{max}\;=\;\frac{2E_{\gamma}^2}{m_e + 2E_\gamma} \;[\textrm{MeV}_{ee}],
\end{equation}
where $(E_e)_{max}$ is the maximum energy transfered by a gamma ray to a recoil electron, $E_\gamma$ is the energy of the gamma-ray and $m_e$ is the mass of the electron.
Next a Gaussian function was fitted to the region of the Compton edge. The QDC channel where the Gaussian distribution reaches 89\% of its height is associated with $(E_e)_{max}$. The \SI{4.44}{MeV} Compton edge produced by the de-excitation of $^{12}$C was used as well as the \SI{2.23}{MeV} Compton edge produced by the de-excitation of $^2$H.

Figure~\ref{fig:qdc_a} shows a QDC callibration spectrum (produced with a \SI{500}{ns} integration window). The narrow peak the furthest to the left in Fig.~\ref{fig:qdc_a} is the pedestal. It is produced when the QDC is made to trigger when there is no current pulse in the detector. The pedestal was produced for calibration purposes by allowing a small fraction of the YAP events to both trigger start and stop. It represents the zero point of the QDC spectrum, so for the energy calibration it is associated with \SI{0}{MeV_{\textit{ee}}}. All the values used in the calibration are shown in Table~\ref{tab:knox_a}.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	QDC(channel)             & 67.5 & 11.43 & 2718 \\
	\hline
	E(MeV)          & 0    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(\textrm{MeV}_{ee})$ & 0    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
   	\captionsetup{width=0.435\linewidth}
	\caption[Analog energy-calibration data.]{Analog energy-calibration data. QDC channels fitted to known Compton edges. See text for details.}
	\label{tab:knox_a}
\end{table}

The x-axis at the top of the panel shows the calibrated energy scale. The bump immediately to the right of the pedestal is produced when this YAP trigger by chance coincides with a random deposition of energy in the NE213 detector. The \SI{2.23}{MeV} and the \SI{4.44}{MeV} Compton edges have been highlighted in green and red. Using the points listed in Table~\ref{tab:knox_a}, a linear calibration fit was made and plotted, see Fig.~\ref{fig:qdc_a} bottom panel. With this fit the QDC spectrum was converted from channels to \si{MeV_{\textit{ee}}}. It can be seen that the uncertainty is greatest for the 2.23 \si{\MeV} Compton edge\footnote{The uncertainty was calculated from the Gaussian fits using error propagation: $$\sigma_\textrm{channel}\;=\;\sqrt{\sigma_{x_0}^2 + \sigma_{k}^2 4\left(\ln0.89\right)^2 },$$ where $k$ is the standard deviation and $x_0$ is the cente of the fit.}.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogResults/Ecall.pdf}
        \caption[Energy calibration of the analog setup]{QDC calibration spectrum. Top panel: PuBe source measured using the NE213 detector. The upper x-axis has been calibrated. Bottom panel: The calibration fit produced with the Knox method. The pedestal is indicated with a red box in both plots.}
    \label{fig:qdc_a}
\end{figure}

\subsubsection{Charge-Comparisson Pulse-Shape Discrimination}
The charge comparisson method was implemented by integrating the pulses in the NE213 detector over a LG window of \SI{500}{ns} and a SG window of \SI{60}{ns}. As described in Sec.~\ref{sec:psd} the constants $a$ and $b$ were tuned to optimize the separation between neutron and gamma bands.




\subsection{Digital}
\subsubsection{Digitizer Specifications and Configuration}
A digitizer is an electronic device which converts analog signals to digital waveforms. A continuous analog signal is approximated by a list of numbers, each representing a single sampling point, along with some additional information such as a time stamp based on a global clock and the channel number of the signal. An acquisition is triggered when the amplitude of the input signal crosses a configurable threshold value.
By this definition, a digitizer is similar to a digital oscilloscope. The main difference is that the oscilloscope has a display and is optimized for portability and realtime diagnostic use, whereas the digitizer is optimized for efficient high-rate data transmission to a computer where further analysis can be carried out, either online or more commonly offline.
The advantage of a digitizer over a traditional analog DAQ system is that it allows the user to process a single data set in multiple ways in order to optimize the parameters of the acquisition, without having to change anything in the physical setup or acquire subsequent data. The disadvantage is that the quality of the discretization of a continuous signal is limited by the sampling rate, dynamic range, resolution and bandwidth of the digitizer as well as the data-transfer rate of the read-out system.


The sampling rate is the frequency at which the digitizer samples a signal. The better the sampling rate the better the digital representation of the original analog signal, see Fig.~\ref{fig:samplingrate}.
The dynamic range is the difference between minimum and maximum voltage the digitizer can record. The resolution defines the number of partitions the dynamic range is divided into and is typically given in bits. The bandwidth determines which signals a digitizer can reproduce without significant alteration. Signals whose fourier expansion contain higher-frequency components will require greater bandwidth\footnote{For example a perfect digitization of a square wave will require an infinite bandwidth, as its fourier expansion is an infinite sum.}. The data-transfer rate defines how fast the digitizer can transfer data to a computer. Low data-transfer rate will lead to greater deadtime.

\begin{figure}[ht]
	\centering
    	\includegraphics[width=0.8\textwidth]{DigitalSetup/samplingrate.pdf}
        \caption[Sampling rate.]{Sampling rate. Illustration of an analog signal sampled with two different sampling rates, $f_0$ and $f_0/2$.}
	    \label{fig:samplingrate} 
\end{figure}

%Sampling rate
The digitizer used in this work is an 8 channel CAEN VX1751 waveform digitizer. It has a sampling rate of \si{1\giga\sample\per\second} in standard mode, but can also be operated in double-edge sampling mode, which disables 4 cannels but increases the sampling rate to \SI{2}{\giga\sample/\second}~\cite{CAEN}. The data presented in this thesis were acquired in standard mode. This resulted in one data point per \si{ns} when digitizing signals. Given a set of sample points, there will always be an infinite number of waveforms that fit the points~\cite{Spectrum}. This is called aliasing. Nyquist's theorem states that if the sampling rate is at least twice as large as the highest frequency component of the signal, aliasing may be avoided~\cite{Spectrum}. An even higher sampling rate is typically desirable. If the sampling rate is too low, important features such as peak location and amplitude will be less well defined, see Fig~\ref{fig:samplingrate}.
In order to reproduce signals accurately, it is not enough to have a high sampling rate. The dynamic range, resolution and bandwidth also need to be considered. 
The VX1751 digitizer has a \SI{1}{\volt} dynamic range, which means that the difference between maximum and minimum voltage is \SI{1}{\volt}. This is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bits, so the \si{1\volt} dynamic range is divided into 1024 bins each of size \SI{0.978}{\milli\volt}~\cite{CAEN}.
The VX1751 has a bandwidth of \SI{500}{\mega\hertz}~\cite{CAEN}. The bandwidth is important because it determines the frequency range of signals that can be digitized without significant attenuation. If the frequency components as obtained from a Fourier expansion of the signal are too high, then the amplitude recorded by the digitizer will be lower than the actual amplitude of the input signal. A rule of thumb for evaluating the bandwidth needed by a given signal is:
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{\textrm{rise}},
\end{equation}
where $T_\textrm{rise}$ is the time it takes the pulse to rise from 10\% to 90\% of peak amplitude and $B$ is the bandwidth for which the signal is attenuated to only 70\% of the original amplitude~\cite{Leo}. Thus, with a \SI{500}{MHz} bandwidth, the VX1751 will cause 70\% attenuation in signals with \SI{0.7}{ns} rise time. Since the signals studied here have rise times on the order of 5-\SI{15}{ns}, bandwidth is not a limitation.

%transfer rate
The digitizer was connected to a computer running Centos 7.4 via an optical link. This connection supports transfer rates of \SI{80}{MB/s}~\cite{CAEN}. However the number of events transfered at a time is also a limiting factor. In this work, the data were transferred on an event-by-event basis, which led to a significant reduction in livetime, see Chap.~\ref{ch:results}. The digitizer is controlled by \textsc{WaveDump} version 3.8.1, published by CAEN under the terms of the GNU General Public License~\cite{WaveDump}. It uses the proprietary digitizer control libraries, also published by CAEN\footnote{The version used in this work was modified to write all data to a single file rather than one file per channel.}. 
\textsc{WaveDump} configures the digitizer according to a text file supplied by the user. The number of data points per trigger is defined globally for all enabled channels. The signal polarity, trigger threshold and baseline offset also need to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a pulse must have in order to be recorded. The baseline offset determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset. The NE213 detector was connected to channel 0 of the digitizer and the YAP detector was connected to channel 1. Both the resulting NE213 and YAP pulses were of negative polarity. A threshold of \SI{-48.8}{mV} was applied to the NE213 detector signals and a threshold of \SI{-9.8}{mV} was applied to the YAP detector signals. A baseline offset of 40\% was inadverdently applied to the NE213 channelthe and an offset of 10\% was applied to  YAP channel. This meant that high amplitude NE213 pulses were clipped (while high amplitude YAP pulses were not). The pulses thus saturated the dynamic range of the digitizer resulting in pulses with a flat top.

For a one hour run with the NE213 detector placed 1.05 m from the PuBe source and one YAP detector connected, data corresponded to $\sim$\SI{120}{GB} text file. The Python library \textsc{Dask} was used for processing the data because it is optimized for processing datasets that are too large to fit in random-access memory. It also runs on all available processor cores~\cite{Dask}. After processing and data reduction, the data were saved as a binary file of size 7.3 GB (in the parquet file format). The Python library \textsc{Pandas} was used for additional processing and the visualization of the reduced data set~\cite{Pandas}\footnote{Python version 3.6.3 was used with Pandas version 0.23.4 and Dask version 1.0.0}.

\subsubsection{Time stamping}
The digital setup needs a method for providing a time stamp for each pulse from the detectors. A global time stamp is provided by the digitizer for each acquired waveform or ``event". However, each event is 1204 ns long, and the pulses do not begin at the exact same point in time within the acquisition window. This is because the trigger clock triggers on the leading edge of pulses and because it runs at only \SI{125}{MHz}, whereas the sampling rate is \SI{1}{GHz}. Consequently it was necessary to precisely determine where in the sampling window the pulse was located. For this purpose, a software-based CFD algorithm was implemented. The algorithm searched the first \SI{40}{ns} before the maximum amplitude of the pulse for the first sampling point to rise to 30\% of the maximum amplitude. Linear interpolation between this sampling point and the previous sampling point enabled a time stamp with resolution better than \SI{1}{ns} to be generated. In Fig.~\ref{fig:cfd_trig}, four pulses from the NE213 detector are plotted centered around their CFD trigger points. Although this time stamp is given with sub-ns precision, the accuracy is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate, resolution and bandwidth. 

\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/goodevents.pdf}
        \caption[Relative timing for NE213 detector pulses of different amplitude]{Relative timing for NE213 detector pulses of different amplitude. A CFD algorithm was used to generate a precise time stamp. A CFD trigger level of 30\% of the maximum amplitude was used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsubsection{Data Selection}
As the digitizer only enforced a threshold, additional selection criteria were applied offline to filter the data set. Since the VX1751 triggered on all enabled channels simultaneously, all empty acquisition windows were discarded. This was done by determining and subtracting a baseline for each waveform. The baseline was determined by averaging over the first \SI{20}{ns} of the waveform. The peak amplitude relative to this new baseline was then determined. A threshold of \SI{24.4}{mV} was enforced removing all pulses of amplitude below this value.

During the digitizer configuration, a baseline offset that was too high was inadvertently applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. Events whose amplitude could not be contained in the available range had their peaks clipped. These 5.7\% of the data were not discarded, but affected the QDC spectrum. The pulse-height spectrum shown in Fig.~\ref{fig:cutoffevents} highlights the clipped events in green.
\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption[Digitized pulse-height spectrum]{Digitized pulse height spectrum from the NE213 detector. Clipped events which reached the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}

Certain events were removed because they caused either the baseline determination, pulse integration or CFD algorithm to produce spurious results. For example, in the case of 2.7\% of all events above trigger threshold, the baseline determination was deemed unsteady, see Fig.~\ref{fig:badevents}~(a). Events were removed when the standard deviation of samples in the baseline-determination window was greater than 2 mV. A subset of the times this happened was because a pulse was located inside the baseline determination window as shown in Fig.~\ref{fig:badevents}~(b). This was identified when the CFD trigger point was located within the first \SI{20}{ns} and happened 0.0016\% of the time. Figure~\ref{fig:badevents}~(c) shows a situation where the CFD triggered so late in the acquisition window that not enough samples followed to carry out the \SI{500}{\nano\second} LG integration. This occurred in 1.1\% of the events. And finally, 0.00066\% of the events were filtered because a peak was immediately  preceded by a smaller peak, see Fig.~\ref{fig:badevents}~(d). Since the CFD algorithm searched the \SI{40}{ns} immediately prior to the peak amplitude, it may have triggered due to the preceeding smaller pulse. All of the identified types of problematic events are summarized in Table~\ref{tab:badevents}.

\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
Description                          & Percentage of events & Discarded \\ \hline
Pulses were clipped                           & 5.7\%                          & No                 \\ \hline
Unstable baseline                             & 2.7\%                        & Yes                \\ \hline
CFD trigger in baseline determination window  & $\ll$0.1\%                      & Yes                \\ \hline
CFD trigger too late for longgate integration & 1.1\%                         & Yes                \\ \hline
CFD algorithm failed                          & $\ll$0.1\%                    & Yes                \\ \hline

\end{tabular}
\caption[Summary of problematic events.]{Summary of problematic events. In general the dataset was very healthy.}
\label{tab:badevents}
\end{table}

\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.87\textwidth]{DigitalSetup/badevents.pdf}
        \caption[Examples of rejected digitized events]{Examples of rejected digitized events. Regions of interest are highlighted with red boxes. (a) The baseline was deemed unstable. (b) A pulse was located inside the baseline-determination window (a subset of (a)). (c) Less than \SI{500}{ns} follow the CFD trigger point, so the LG integration could not be carried out. (d) Closely adjacent pulses caused the CFD algorithm to fail.}
    \label{fig:badevents} 
\end{figure}
\newpage
\subsubsection{Energy Calibration}\label{sec:Ecal_D}
The energy calibration of the digital setup was carried out in a similar manner to the analog setup, using the Knox method. The pulses were integrated digitally over the exact same gate lengths used in the analog setup, namely 60 and 500 ns starting 25 ns before the CFD trigger. A major difference compared to the analog setup was in the baseline determination. For the analog setup, the pedestal was needed by the energy calibration to account for and subtract any baseline offset. It acted as a global baseline subtraction. This was not necessary in the digital setup since the baseline is subtracted on an event by event basis during the initial data processing. 

Both the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges produced by the PuBe source as well as a \SI{1.33}{MeV} Compton edge from a $^{60}$Co source were used for the calibration. the calibration points are listed in Table~\ref{tab:knox_d}. In Fig.~\ref{fig:D_QDC}, the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges are marked in purple and orange respectively. The $^{60}$Co source has Compton edges at \SI{1.17}{\MeV} and \SI{1.33}{\MeV}, but due to energy resolution only one edge is visible.  As with the analog setup the uncertainty is the greatest for the \SI{2.23}{\MeV} Compton edge.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	channel (mV$\cdot$ns)            & 3791 & 7080  & 15481  \\
	\hline
	E(MeV)          & 1.33    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(\textrm{MeV}_{ee})$ & 0.96    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
   	\captionsetup{width=0.435\linewidth}
	\caption[Digital energy-calibration data.]{Digital energy-calibration data. Digitizer pulse integration channels fitted to known Compton edges. See text for details.}
	\label{tab:knox_d}
\end{table}

The baseline shift on the NE213 channel offset was set too high, restricting the range available to negative pulses to only \SI{0.6}{V}. This affected the 4.44 \si{\MeV} Compton edge. In spite of this problem, Fig.~\ref{fig:D_QDC} shows that the calibration points still follow a linear trend within uncertainty. The fit parameters were used to produce the calibrated x-axis in the upper panel.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.8\textwidth]{DigitalResults/Ecall.pdf}
        \caption[Energy calibration of the digital setup]{The \SI{1.33}{\MeV} Compton edge of the $^{60}Co$ source and the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} Compton edges of the PuBe source have here been used to perform an energy calibration. Due to the event-by-event baseline determination, bin 0 corresponds to \SI{0}{\MeV}$_{ee}$}
    \label{fig:D_QDC}
\end{figure}

\subsubsection{Charge-Comparison Pulse-Shape Discrimination}
Two approaches to PSD were explored with the digitizer. The charge comparison method was implemented by integrating the waveforms over 60 and \SI{500}{ns} gates respectively, starting \SI{25}{ns} before the CFD trigger. The exact same gate lengths and timing employed by the analog setup were used here to facilitate a direct comparison to two setups. In addition, the separation between neutron and gamma-ray distributions was linearized by fine tuning the parameters $a$ and $b$ as shown in Fig~\ref{fig:psd_sketch}.

\section{Convolutional Neural-Network}\label{sec:cnn}
The biggest advantage a digitizer offers is that it records the entire pulse rather than just extracting a few parameters from it. With the entire waveform available, PSD can be approached in ways that are not feasible with analog electronics. One such approach is to apply an artificial neural network to the task of discriminating neutrons from gamma-rays. 
Artificial neural networks are function approximators, which use learned parameters called weights to perform a specified task. There are a variety of different network types. For image classification tasks such as PSD, convolutional neural networks (CNNs) set the gold standard. Training neural networks to perform neutron/gamma-ray PSD is not a new approach. It has been successfully implemented in a number of studies for various scintillators, see~\cite{Griffiths}. The key difference between what has previously been done and what is done within this thesis lies in the manner the training data was selected.

\subsection{Training dataset}
To train a network to discriminate between neutrons and gamma-rays, a set of digitized waveforms along with labels defining the species of each waveform is needed. One approach would be to create a precise simulation and train the network on the simulated data. Then the labels are known with certainty to represent the waveforms to which they are assigned. On the other hand, the simulated data must be an excellent representation of actual detector signals. This also implies that new simulations will be needed for new detectors. Another approach is to use different PSD techniques for labeling data as either neutrons or gamma-rays. This is the approach taken by  Griffiths et~al~\cite{Griffiths}. They label their training data by plotting the number of samples within a given pulse that surpasses a certain threshold as a function of peak amplitude, and then making a cut to separate neutrons from gamma-rays. This approach is effective as long as the model used to generate the training data is not systematically mislabeling a certain type of pulses, for example pulses in a certain energy range. 

The approach taken here has been to take advantage of the extra information given by the ToF spectrum. The ToF spectrum provides access to a labeled set of neutron and gamma-ray pulses in the form of the neutron bump and the gamma flash, see Fig~\ref{fig:CNN_data}. The downside of this approach is that the ToF spectrum will also contain random coincidences, which means that the training data will contain some neutrons mistakenly labeled as gamma-rays and vice versa. Since the background is composed of random coincidences and makes up only a small amount of the total training data, it is anticipated that the false neutrons/gamma-rays will not cause the network to systematically misclassify, but merely slow down the training.

\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_data.pdf}
        \caption[Selection of training and validation data]{Selection of training and validation data using ToF information. Gamma flash events are labeled 0 and events from the neutron bump are labeled 1.}
    \label{fig:CNN_data} 
\end{figure}

\subsection{How It Works}
Convolutional neural network use kernels of weights to extract features from an input image. The essential features of a kernel is the size, the stride and the weights. Figure~\ref{fig:CNN_frame1} shows an example of a kernel being applied to an input vector $\vec{x}$. When applying the network to PSD, $\vec{x}$ will be a digitized current pulse from the NE213 detector. Here, the convolution is performed with stride 2, i.e. the kernel is moved across the input in steps of 2. The kernel acts on 3 elements in 1 vector, so it has dimensions 1$\times$3. The elements of the kernel are called weights. For every element in $h$ a bias weight $b$ is also added. These are the parameters which will be optimized through training. The kernel is scanned across $\vec{x}$ and a new vector or feature map $\vec{h}$ is produced. Each element of $\vec{h}$ is provided by a function $\phi$ which takes the scalar product of the kernel and a segment of $\vec{x}$ as input, see Fig.~\ref{fig:CNN_frame1}. 
A common choice of activation function for CNNs is the rectified linear unit, ReLU given by:
\begin{equation}
	\textrm{ReLU}(x) = 
	\begin{cases}
    	0, & \text{if}\ x<0 \\
    	x, & \text{otherwise}
    \end{cases}
\end{equation}

%talk about act function

\begin{figure}[h!]
    \centering
        \includegraphics[width=0.85\textwidth]{DigitalSetup/CNN_frame1.pdf}
        \caption[Convolution of a vector and a kernel]{Convolution of a vector and a kernel. Connections from the bias node are marked with a dashed line.}
    \label{fig:CNN_frame1} 
\end{figure}

Convolutional neural networks typically employ multiple layers each containing multiple filters, see Fig.~\ref{fig:CNN_frame2}. Note that Fig~\ref{fig:CNN_frame2} is drawn to illustrate how a CNN works and does not reflect the exact architecture of the network employed in this thesis.  The first operation carried out in this network is marked with \circled{1}. 4 convolutional filters are applied to the input vector to produce 4 new feature maps, in exactly the same manner as was shown in Fig.~\ref{fig:CNN_frame1}. The job of  each filter is to highlight simple features in the input vector.

The next operation, labeled \circled{2} is also convolutions. This time 4 kernels are scanned across the red green and blue feature map in the preceeding layer. Each of the four filters are have dimension $3\times2$, meaning that they operate on two elements in each feature map simultaneously, in order to extract more complex information. This produces the four new feature maps $\vec g_1$, $\vec g_2$, $\vec g_3$, $\vec g_4$. The next operation, labeled \circled{3} is a flattening of the four feature maps into a single vector $\vec f$. In the final operation, labeled \circled{4}, the output $y$ of the network is given by an activation function $\phi_{\textrm{out}}$, which takes a linear combination of the elements of $\vec f$ as input. For binary classification problems, a common choice of activation function $\phi_{\textrm{out}}$ is the logistic function, which is bounded between 0 and 1:
\begin{equation}
	f(x) = \frac{1}{1-e^{-x}}.
\end{equation}
This function allows the output to be interpreted as a probability of the waveform representing a neutron.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_frame2.pdf}
        \caption[Two-layer convolutional neural network.]{Two-layer convolutional neural network. Each of the feature maps in the convolutional layers are produced with a unique kernel, and has been given its own color. For readability only connections to the red and pink feature maps are shown. The other feature maps are connected in the same manner. Likewise bias nodes have been omitted for readability.}
    \label{fig:CNN_frame2} 
\end{figure}

The output of the network will be useless unless the weights have been properly trained to extract relevant features. The network is trained through a simple yet incredibly powerful method called backpropagation. In backpropagation the derivative of an error function with respect to each weight in the network is found through repeated use of the chain rule. These derivatives are then used to perform small adjustments to each of the weights. The procedure is repeated a number of times until, if the network architecture and training data were sufficient, the network has learned how to solve its task. There are many variations of this algorithm, often incorporating momentum terms or calculating the error over a batch of many patterns at a time.
The error function used in this work is called binary cross-entropy error. It is commonly applied to binary classification tasks and is given by:
\begin{equation}
	E = -\frac{1}{N}\sum_n\left(	d_n\log(y(\vec x_n))+(1-d_n)\log(1-y(\vec x_n))	\right).
\end{equation}
This function calculates the average error $E$ over $N$ input vectors $\vec x_n$ with labels $d_n$. $y_n$ is the output of the network. In the case of neutron/gamma-ray discrimination this will be $N$ digitized waveforms. if $x_n$ represents a neutron, then the label $d_n=1$ and the second term disappears. If instead $x_n$ represents a gamma-ray then $d_n=0$ and the first term disappears.
After propagating $N$ waveforms through the network the weights can be updated via:
\begin{equation}
	\omega^{t+1}_i = \omega^{t}_i - \eta\frac{\partial E^t}{\partial \omega_i},
\end{equation}
where $\omega_i$ is weight $i$ in the network and $E$ is the error function. $t$ is the training iteration and corresponds to $N$ waveforms having been propagated through the network in order to calculate $E$. The constant $\eta$ is a scaling factor called the learning rate. It scales the corrections down to avoid overshooting optimal weights. Training the network is then a matter of finding weights $\omega$ which minimize $E$.




\subsection{Implementation of the Network}
A CNN was implemented using \textsc{Keras} version 2.2.4\footnote{\textsc{Keras} is a high level framework for for constructing deep-learning models in \textsc{Python}. It can be run using different backends for carrying out operations on tensors. Here \textsc{Tensorflow} 1.12.0 was used as backend.}\cite{keras}. The main features of the chosen architecture are summarized in Table~\ref{tab:architecture}. In addition to convolutional layers, this network also contains max pooling layers. Max pooling layers reduce the size of each feature map individually by passing on only the maximum value of a given neighbourhood of the input vector. With a size of 2 and stride of 2, input vectors are reduced to half their size, see Fig.~\ref{fig:maxpooking}. Due to the kernel size and stride as well as the max pooling, each node in the flattened layer is indirectly connected to a large number of samples in the input layer. Both convolutional layers employ the $ReLU$ activation function, while the final layer applies the logistic function.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.25\textwidth]{DigitalSetup/maxpooling.pdf}
        \caption[The Max pooling principle]{The Max pooling principle.}
    \label{fig:maxpooking} 
\end{figure}

\begin{table}[h]
\center
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{\textbf{Input layer}: vector size 300}                                            \\ \hline
\textbf{Hidden layers}       & Kernel size & N kernels & Stride & Activation function \\ \hline
\textbf{Convolutional layer} & 9$\times$1           & 10                & 4             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2$\times$1           & -                 & 2             & -                   \\ \hline
\textbf{Convolutional layer} & 5$\times$10           & 16                & 2             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2$\times$1           & -                 & 2             & -                   \\ \hline
\multicolumn{5}{|c|}{\textbf{Fully connected} size 144}
\\ \hline
\multicolumn{5}{|c|}{\textbf{Output layer} size 1, \textbf{activation function}: Sigmoid}               \\ \hline
\end{tabular}
\caption{Table of the essential parameters in the CNN.}
\label{tab:architecture}
\end{table}



\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption[Training and validation accuracy of the CNN.]{Training and validation accuracy of the CNN. The dashed line indicates the epoch corresponding to the chosen model.}
    \label{fig:CNN_training} 
\end{figure}

The network was trained on events from the gamma-ray and neutron ToF peaks of a 75 minute data set. 300 samples (or equivalently ns) from each pulse were used starting \SI{20}{ns} prior to the CFD trigger point. 75\% of the pulses or 3018 events from the neutron bump and 3018 events from the gamma-flash were used as labeled training data while the remaining 25\% were used to evaluate the model, see Fig.~\ref{fig:CNN_training}. Here the accuracy of the model, defined as the fraction of correctly labeled events, is plotted as a function of $Epoch$, with $Epoch$ defined as the number of times the network has trained on the entire training dataset. The blue curve shows the fraction of correctly labeled pulses achieved on the training data as a function of iteration, while the red curve shows the same thing on the validation data. The red curve varies more since it represents a smaller dataset. As both training and validation data contains some fraction of incorrectly labeled background events the model is not expected to reach 100\% accuracy on either data set. This plot shows that although the performance on the training set keeps increasing, the performance on the validation set quickly levels out. For this reason the model achieved at iteration 53 is used. The network is still learning beyond iteration 53, but it is no longer learning features that generalize to the validation data, it is instead overfitting to noise in the training data.








\end{document}