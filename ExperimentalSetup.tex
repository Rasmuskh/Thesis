\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Method}
\section{Experimental Infrastructure}
\subsection{Source-Testing Facility}
The Source-Testing Facility (STF) at the Division of Nuclear Physics in Lund, Sweden, is a fully equipped user facility for the characterization of detectors, shielding and sources~\cite{Messi}.
The operation of the STF is a collaborative effort between the Division of Nuclear Physics and the ESS Detector Group. It is employed for the development of detectors for ESS and industry. The STF offers easy and reliable access to actinide/Be fast-neutron sources and gamma-ray sources. Additionally, fast neutrons can be moderated to lower energies with various materials. The STF is divided into two areas, see Fig.~\ref{fig:STF}. The user area contains data-acquisition systems (DAQs), workstations as well as a wide range of electronics modules and detectors. The smaller interlocked area is where measurements are carried out. This area contains an array of shielding materials as well as a dedicated neutron-tagging setup based upon the Aquarium.

\begin{figure}[ht]
	\center
    	\includegraphics[width=0.6\textwidth]{AnalogSetup/STF.pdf}
	\caption[The Source-Testing Facility.]{The Source-Testing Facility. The smaller room to the left is the interlocked area where measurements are carried out. The user area to the right contains DAQ setups, workstations and storage. Figure from Ref.~\cite{Messi}.}
	\label{fig:STF}
\end{figure}

\subsection{The Aquarium}
The fast-neutron source may be located inside a 140$\times$140$\times$140 \si{\cm}${}^\text{3}$ tank of water referred to as the Aquarium, see Fig.~\ref{fig:aquarium}. The Aquarium has four horizontal cylindrical beam ports intersecting at a central volume. A source and up to four gamma-ray detectors may be placed within the central volume.  The beam ports are air filled, allowing neutrons and gamma-rays to reach a neutron/gamma-ray detector placed next to the aquarium without passing through shielding. Each of the beam ports can be plugged when not in use.
By moderating and absorbing the fast neutrons, the water tank both provides shielding from the sources and gives rise to a distinguishable gamma-ray energy of \SI{2.23}{MeV} produced in the de-excitation of the deuteron via the $^{\text{1}}$H($n$,$\gamma$)$^{\text{2}}$H$^*$ reaction. This gamma-ray is useful when performing energy calibrations, see Sec.~\ref{sec:Ecal_A} and Sec.~\ref{sec:Ecal_D}. 
An actinide/Be source may be positioned on the central vertical axis of the Aquarium and can be raised to the same height as the ports for ToF measurements. In a ``lowered" or ``parked" position, there is no direct line-of-sight through air from the source through the ports. The four dedicated gamma-ray detectors are located near the source, but are raised slightly to allow a direct line-of-sight from source through the ports, see Fig.~\ref{fig:aquarium}.
\begin{figure}[ht]
	\center
    	\includegraphics[width=\textwidth]{AnalogSetup/aquarium_combined.pdf}
	\caption[The Aquarium.]{The Aquarium. Left panel: Oblique view with the neutron/gamma-ray detector in front of one of the horizontal cylindrical beam ports. Lines-of-sight from the source out of two of the ports are indicated with arrows. The source and the four gamma-ray detectors are located in the tubes at the center of the aquarium. Right panel: Top view with the source and detectors indicated by arrows.}
	\label{fig:aquarium}
\end{figure}




\subsection{Radiation Sources}
Two radiation sources were used in this work. The actinide/Be fast-neutron and gamma-ray source $^\text{238}\text{Pu}/^\text{9}\text{Be}$ (referred to as Pu/Be) was used for tagging fast neutrons and for energy calibration of the neutron/gamma-ray detector. As shown in Sec.~\ref{sec:freeNeutrons}, the Pu/Be source produces both a cascade of low-energy photons and fast neutrons, which $\sim$55\% of the time are accompanied by the emission of a \SI{4.44}{\MeV} gamma-ray. The source has been measured to produce approximately $\text{2.99}\cdot\text{10}^\text{6}$ neutrons per second~\cite{Scherzinger:2017}. The pure gamma-ray source $^\text{60}\text{Co}$ was used for detector calibration. $^\text{60}\text{Co}$ decays to excited states of $^\text{60}\text{Ni}$ via beta decay. De-excitation of $^\text{60}\text{Ni}$ will result in gamma-rays of energies \SI{1.17}{MeV} or \SI{1.33}{MeV}~\cite{Nudat}.



\subsection{Fast-Neutron and Gamma-Ray Detectors}
As ToF depends on the accurate timing of $\gamma$n and $\gamma\gamma$ pairs, it is essential to detect both particle species with accurate timing. In this work, two different detectors were used for detecting neutrons and gamma-rays from the sources. Both of which produce pulses with negative polarity. A liquid organic NE213 detector was used to detect both gamma-rays and neutrons while an inorganic Cerium-doped Yttrium Aluminum Perovskite crystal (YAP) detector was used to detect gamma-rays. 

\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/detectors.pdf}
        \caption[Photographs of the detectors.]{Photographs of the detectors. Left: NE213 detector. Right: YAP detector.}
    \label{fig:detectors}
\end{figure}

Since its introduction in the early 1960s, the NE213 liquid organic scintillator has become the gold standard for fast-neutron detection due to its excellent neutron/gamma-ray discrimination capabilities and high detection efficiency. 
The drawbacks of this scintillator are that it is toxic and highly volatile with a flash point of 26$\text{\degree}$C. The NE213 used here was produced by Nuclear Enterprises. It is equivalent to EJ301, currently produced by Eljen Technology~\cite{Eljen}. The decay times of the first three scintillation components are \SI{3.16}{ns}, \SI{32.3}{ns} and \SI{270}{ns}~\cite{Eljen}. It is contained in a 122$\times$122$\times$179 \si{\mm}$^\text{3}$ volume which is connected to a photomultiplier tube via a lightguide\footnote{This detector was constructed by Johan Sj√∂gren as part of his thesis work in 2009-2010~\cite{sjogren}.}.

Near the source, four YAP detectors are placed. The YAP detectors are mounted on photomultiplier tubes. These inorganic scintillators are largely insensitive to neutrons and provide excellent timing of gamma-rays. As the YAP detectors are located closer to the source they experience a significantly higher gamma-ray flux than the NE213 detector does. This makes time resolution and decay time critical factors in their performance. The scintillation light has a decay time of $\sim$27 ns~\cite{Scionix}. Howevever, it takes $\sim$\SI{100}{ns} for signals to return to the original baseline level. This means that the detector can handle count rates in the low MHz range without significant pileup. To simplify the analysis and limit the data rates, only one YAP detector was used in this project.

\section{Signal Processing}
Two different experimental systems were employed. These experimental setups differed only in the DAQ system used and shared the same physical setup of detectors, shielding and radiation source. By sending the detector signals through an active splitter, both DAQs could be run in parallel on the exact same detector signals. The first setup employed NIM modules to process signals and generate a trigger decision. VME modules were used to digitize the timing and charge characteristics of the signals. The digitized data were transferred to a computer where they were saved and plotted in real time. Since this setup did most of the data processing via analog electronics, it will be referred to as the analog setup. The second setup was based on a digitizer which recorded detector signals as digital waveforms for offline analysis. Since all of the processing of the signals was performed digitally, this setup will be referred to as the digital setup. Figure~\ref{fig:DAQ} contrasts the analog and digital setups. The analog setup is composed of a variety of NIM and VME modules as well as a large number of LEMO cables. The digital setup is composed of a single digitizer, which fits in a single VME slot. The digital setup is thus far more compact spatially.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.95\textwidth]{AnalogSetup/setup_flowchart.pdf}
        \caption[Schematic of the experimental setup.]{Schematic of the experimental setup. Green box: the digital setup. Yellow box: the analog setup.}
    \label{fig:setup}
\end{figure}


\begin{figure}[h]
    \centering
        \includegraphics[width=0.9\textwidth]{AnalogSetup/DAQ.pdf}
        \caption[Data-acquisition systems.]{Data-acquisition systems. The analog DAQ rack is shown to the left. Digitization modules are highlighted by the orange arrow, and the power supply is indicated by the blue arrow. A closeup of the analog electronics is shown in the red box. The single digitizer module is shown highlighted in green to the right.}
    \label{fig:DAQ}
\end{figure}

\subsection{Analog}
The detector signals were replicated by a fan-in-fan-out (FIFO) module and copies were sent to both the analog and the digital DAQ setups, see Fig.~\ref{fig:setup}. On the analog side, the signals were processed before time and energy information were sent through an optical link to a computer running Centos 7.3 where it was written to hard drive. 

\subsubsection{Constant-Fraction Discriminators}
The YAP and NE213 pulses were sent to constant-fraction discriminators (CFDs), labeled \circled{1} in Fig.~\ref{fig:setup}. CFDs were used because ToF measurements require precision timing of the start and stop signals. Simply triggering on the leading edge of pulses will lead to ``time walk" for similarly shaped pulses of varying amplitudes. Time walk means that pulses of the same shape but different amplitude will result in triggers at different times, see Fig.~\ref{fig:discriminator}. As can be seen, the smaller pulse passes the threshold at a later time than the larger pulse. By instead triggering on the point where a pulse reaches a certain fraction of its peak amplitude, time walk can nearly be eliminated~\cite{Leo}. This may be achieved by dividing or copying the signal, inverting one copy and delaying the other. The pulses are then added and the zero crossing of the summed signal corresponds to a fraction of the input pulse, see Fig.~\ref{fig:cfd}. Changing the amplitude of the incoming signal will have the same effect on both copies of the signal. Consequently the CFD trigger time, defined by the zero-crossing, is amplitude independent. The output of the CFD was a logic pulse representing the trigger time of the CFD. CFDs were also used to enforce an amplitude threshold of \SI{94.6}{mV} for the NE213 detector and \SI{25.0}{mV} for the YAP detector. Only pulses above the thresholds generated logic pulses.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.7\textwidth]{AnalogSetup/threshold.pdf}
        \caption[Time walk resulting from leading-edge triggering.]{Time walk resulting from leading-edge triggering. Figure adapted from Ref.~\cite{Rofors}.}
    \label{fig:discriminator}
\end{figure}

\begin{figure}[h]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogSetup/cfd_emil.pdf}
        \caption[CFD trigger principle.]{CFD trigger principle. Figure from Ref.~\cite{Rofors}.}
    \label{fig:cfd}
\end{figure}



\subsubsection{Trigger Logic}
The NE213 logic pulses generated by the CFD were sent to a latch (labeled \circled{2} in Fig.~\ref{fig:setup}), which allowed a single pulse to pass at a time and blocked any further pulses until it received a reset signal. The pulses that made it through were passed on to the data-acquisition modules presented below. The reset signal was given after the data-acquisition modules had finished digitization ($\sim$\SI{10}{\micro s}) and the data has been written to a computer via optical link ($\sim$\SI{350}{\micro s}). The amount of time the latch was closed is called the deadtime. This is because the system is unable to process new events (effectively dead) during this time. The deadtime was measured by an uninhibited and an inhibited scaler connected to a pulser. A scaler is a module which simply counts the number of logic pulses it receives. The pulser incremented the scalers at a specific frequency. Whenever the latch was closed, the inhibited scaler was no longer incremented. This made it possible to calculate the livetime as the ratio of inhibited scaler counts to uninhibited scaler counts every time the scalers were read. Note that although the setup contained both YAP and NE213 detectors, only the NE213 detector was used to trigger the acquisition. Since ToF measurements require both a start and stop signal, there can at most be as many coincidences as there are pulses in the detector with the lowest count rate. The NE213 detector experiences the lowest count rate. Thus, the deadtime was minimized by triggering the acquisition on the NE213 detector.

\subsubsection{Time-to-Digital Converters}
A logic signal was also sent from the latch to a time-to-digital converter (TDC). This module is labeled \circled{3} in Fig.~\ref{fig:setup}. The TDC charges a capacitor with a constant current once a start signal is received, and stops once the stop signal is received. The start signal is the event logic pulse from the NE213 detector and the stop signal is a delayed event pulse from the YAP detector. The amplitude of the capacitor-discharge pulse is proportional to the time between the start and stop signals~\cite{CAENTDC}. An analog-to-digital converter within the TDC digitizes the pulse and sends the resulting number to the computer via an optical link.
\begin{figure}[h]
	\centering
    	\includegraphics[width=0.8\textwidth]{AnalogSetup/Tcal.pdf}
        \caption[TDC calibration spectrum.]{TDC calibration spectrum. Top panel: Each peak represents a different delay value for the stop pulse. Bottom: A linear fit applied to the TDC and delay values.}
	    \label{fig:Tcal} 
\end{figure}

The raw values provided by the TDC had units of TDC channels, so a calibration was necessary to convert them into ns. By triggering the start of the TDC with a delayed version of the same signal used to stop the TDC, a very sharp peak was produced. This peak was then shifted over known intervals by increasing delay-cable length to produce the calibration data shown in Fig.~\ref{fig:Tcal}. Since the location of $t=0$ is arbitrary, the important result is the calibration coefficient \SI{0.28}{\nano\second/channel} which was used to convert timing data from TDC channels to \si{ns}.

\subsubsection{Charge-to-Digital Converters}\label{sec:Ecal_A}
The pulses that made it through the latch were used to generate \SI{60}{\ns} and \SI{500}{\ns} logic pulses with the first \SI{25}{\ns} preceding the CFD trigger point. These pulses acted as integration ``windows'' or ``gates" for the charge-to-digital converters (QDCs). In Fig.~\ref{fig:setup}, these modules are labeled \circled{4}. Copies of the analog current pulses were also sent to the QDC modules. Each module carried out an integration for the duration of the applied gate. The resulting charges provided a measure of the energy deposited in the detector by a given pulse on two different timescales. The QDC modules sent their outputs via optical link to a computer where they were written to the hard drive, see Fig.~\ref{fig:setup}.

The charge integration performed by the QDC modules is a measure of the energy deposited in a detector. In the NE213 detector, neutrons interact primarily via scattering from $^1$H while gamma-rays interact primarily with atomic electrons. It is customary to calibrate a fast-neutron detector with gamma-ray sources, employing the ``electron-equivalent energy". Electron-equivalent energy corresponds to the amount of energy deposited by an electron. As gamma-rays interact with atomic electrons, they result in electron-equivalent deposited energies. One way of performing this energy calibration is the Knox method of examining the Compton edge corresponding to monoenergetic gamma-rays~\cite{Nilsson}. The maximum energy transferred by a gamma-ray to a recoil electron is given by:
\begin{equation}
	(E_{e})_{max}\;=\;\frac{2E_{\gamma}^2}{m_e + 2E_\gamma} \;[\textrm{MeV}_{ee}],
\end{equation}
where $(E_e)_{max}$ is the maximum energy transfered by a gamma-ray to a recoil electron, $E_\gamma$ is the energy of the gamma-ray and $m_e$ is the mass of the electron.
A Gaussian function may then fitted to the region of the Compton edge. The QDC channel where the Gaussian distribution reaches 89\% of its height is associated with $(E_e)_{max}$. The \SI{4.44}{MeV} Compton edge produced by the de-excitation of $^{12}$C was used together with the \SI{2.23}{MeV} Compton edge produced by the de-excitation of $^2$H for calibration purposes.

Figure~\ref{fig:qdc_a} shows a QDC calibration spectrum produced with a \SI{500}{ns} integration window. The narrow peak the furthest to the left in Fig.~\ref{fig:qdc_a} is the pedestal. It is produced when the QDC is made to trigger when there is no current pulse in the detector. The pedestal was produced for calibration purposes by allowing a small fraction of the YAP events to trigger the DAQ. It represents the zero point of the QDC spectrum, so for the energy calibration it is associated with \SI{0}{MeV_{\textit{ee}}}. All the values used in the calibration are shown in Table~\ref{tab:knox_a}.

\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	QDC(channel)             & 67.5 & 1272.1 & 2718.9 \\
	\hline \hhline{|=|=|=|=|}
	E(MeV)          & 0    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}(\textrm{MeV}_{ee})$ & 0    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
	\caption[Analog energy-calibration data.]{Analog energy-calibration data. QDC channels fitted to known Compton edges. See text for details.}
	\label{tab:knox_a}
\end{table}

The x-axis at the top of the panel shows the calibrated energy scale. The bump immediately to the right of the pedestal is produced when the YAP trigger coincides by chance with a random deposition of energy in the NE213 detector. The \SI{2.23}{MeV} and the \SI{4.44}{MeV} Compton edges have been highlighted in purple and orange. Using the points listed in Table~\ref{tab:knox_a}, a linear calibration fit was made and plotted, see Fig.~\ref{fig:qdc_a} (bottom panel). With this fit, the QDC spectrum was converted from channels to \si{MeV_{\textit{ee}}}. It can be seen that the uncertainty is greatest for the 2.23 \si{\MeV} Compton edge.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.8\textwidth]{AnalogResults/Ecall.pdf}
        \caption[QDC calibration of the analog setup.]{QDC calibration of the analog setup. Top panel: Pu/Be spectrum measured using the NE213 detector. The upper x-axis has been calibrated. Bottom panel: The calibration fit produced with the Knox method. The pedestal is indicated with a red box in both plots.}
    \label{fig:qdc_a}
\end{figure}

\subsubsection{Charge-Comparison Pulse-Shape Discrimination}
The CC method was implemented by integrating the pulses in the NE213 detector over a LG window of \SI{500}{ns} and a SG window of \SI{60}{ns}. As described in Sec.~\ref{sec:psd}, the constants $a$ and $b$ were tuned to optimize the separation between the neutron and gamma-ray bands.




\subsection{Digital}
\subsubsection{Digitizer Specifications and Configuration}
A digitizer is an electronic device which converts analog signals to digital waveforms. A continuous analog signal is approximated by a list of numbers, each representing a single sampling point, along with additional information such as a time stamp based on a global clock and the channel number of the signal. An acquisition is triggered when the amplitude of the input signal crosses a configurable threshold value.
By this definition, a digitizer is similar to a digital oscilloscope. The main difference is that the oscilloscope has a display and is optimized for portability and realtime diagnostic use, whereas the digitizer is optimized for efficient high-rate data transmission to a computer where further analysis can be carried out, either online or more commonly offline.
The advantage of a digitizer over a traditional analog DAQ system is that it allows the user to process a single data set in multiple ways in order to optimize the parameters of the acquisition, without having to change anything in the physical setup or acquire subsequent data. The disadvantage is that the quality of the discretization of a continuous signal is limited by the sampling rate, dynamic range, resolution and bandwidth of the digitizer as well as the data-transfer rate of the read-out system. 
The sampling rate is the frequency at which the digitizer samples a signal. The higher the sampling rate the better the digital representation of the original analog signal, see Fig.~\ref{fig:samplingrate}.
The dynamic range is the difference between the minimum and maximum voltage the digitizer can record. The resolution defines the number of partitions the dynamic range is divided into and is typically given in bits. The bandwidth determines which signals a digitizer can reproduce without significant alteration. Signals whose Fourier expansion contain higher-frequency components will require greater bandwidth\footnote{For example, the perfect digitization of a square wave will require an infinite bandwidth, as its Fourier expansion is an infinite sum.}. The data-transfer rate defines how fast the digitizer can transfer data to a computer. Low data-transfer rate will lead to greater deadtime.

\begin{figure}[ht]
	\centering
    	\includegraphics[width=0.8\textwidth]{DigitalSetup/signals.pdf}
        \caption[Sampling rate.]{Sampling rate. Illustration of an analog signal sampled with two different sampling rates, $f_0$ and $f_0/2$.}
	    \label{fig:samplingrate} 
\end{figure}

%Sampling rate
The digitizer used in this work is an 8 channel CAEN VX1751 waveform digitizer. It has a sampling rate of \si{1\giga\sample\per\second} in standard mode, but can also be operated in double-edge sampling mode, which disables 4 cannels but increases the sampling rate to \SI{2}{\giga\sample/\second}~\cite{CAEN}. The data presented in this thesis were acquired in standard mode. This resulted in one data point per \si{ns} when digitizing signals. Given a set of sample points, there will always be an infinite number of waveforms that fit the points~\cite{Spectrum}. This is called aliasing. Nyquist's theorem states that if the sampling rate is at least twice as large as the highest frequency component of the signal, aliasing may be avoided~\cite{Spectrum}. An even higher sampling rate is typically desirable. If the sampling rate is too low, important features such as peak location and amplitude will be less well defined, see Fig~\ref{fig:samplingrate}.
In order to reproduce signals accurately, it is not enough to have a high sampling rate. The dynamic range, resolution and bandwidth also need to be considered. 
The VX1751 digitizer has a \SI{1}{V} dynamic range, which means that the difference between maximum and minimum voltage is \SI{1}{V}. This is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bits, so the \SI{1}{V} dynamic range is divided into 1024 bins each of size \SI{0.978}{\milli\volt}~\cite{CAEN}.
The VX1751 has a bandwidth of \SI{500}{\mega\hertz}~\cite{CAEN}. The bandwidth is important because it determines the frequency range of signals that can be digitized without significant attenuation. If the frequency components as obtained from a Fourier expansion of the signal are too high, then the amplitude recorded by the digitizer will be lower than the actual amplitude of the input signal. A rule of thumb for evaluating the bandwidth needed by a given signal is:
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{\textrm{rise}},
\end{equation}
where $T_\textrm{rise}$ is the time it takes the pulse to rise from 10\% to 90\% of peak amplitude and $B$ is the bandwidth for which the signal is attenuated to only 70\% of the original amplitude~\cite{Leo}. Thus, with a \SI{500}{MHz} bandwidth, the VX1751 will cause 70\% attenuation in signals with \SI{0.7}{ns} rise time. Since the signals studied here have rise times on the order of $5\--$\SI{15}{ns}, bandwidth is not a limitation.

%transfer rate
The digitizer was connected to a computer running Centos 7.4 via an optical link. This connection supports transfer rates of \SI{80}{MB/s}~\cite{CAEN}. However the number of events transfered at a time is also a limiting factor. In this work, the data were transferred on an event-by-event basis, which led to a significant reduction in livetime, see Chap.~\ref{ch:results}. The digitizer is controlled by \textsc{WaveDump} version 3.8.1 published by CAEN under the terms of the GNU General Public License~\cite{WaveDump}. It uses the proprietary digitizer control libraries, also published by CAEN\footnote{The version used in this work was modified to write all data to a single file rather than one file per channel.}. 
\textsc{WaveDump} configures the digitizer according to a text file supplied by the user. The number of data points per trigger is defined globally for all enabled channels. The signal polarity, trigger threshold and baseline offset also need to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a pulse must have to be recorded. The baseline offset determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset. The NE213 detector was connected to channel~0 of the digitizer and the YAP detector was connected to channel~1. Both the resulting NE213 and YAP pulses were of negative polarity. An amplitude threshold of \SI{48.8}{mV} was applied to the NE213 detector signals and a threshold of \SI{9.8}{mV} was applied to the YAP detector signals. A baseline offset of 40\% was inadvertently applied to the NE213 channel and a baseline offset of 10\% was applied to the YAP channel. This meant that high amplitude NE213 pulses were clipped while high amplitude YAP pulses were not. The pulses thus saturated the dynamic range of the digitizer resulting in a small subset of large pulses with a flat top.

For a one hour run with the NE213 detector placed 1.05 m from the Pu/Be source and one YAP detector connected, data corresponded to $\sim$\SI{120}{GB} text file. The Python library \textsc{Dask} was used for processing the data because it is optimized for processing datasets that are too large to fit in random-access memory. It also runs on all available processor cores~\cite{Dask}. After processing and data reduction, the data were saved to a binary file of size 7.3 GB . The \textsc{Python} library \textsc{Pandas} was used for additional processing and the visualization of the reduced data set~\cite{Pandas}\footnote{\textsc{Python} version 3.6.3 was used with \textsc{Pandas} version 0.23.4 and \textsc{Dask} version 1.0.0.}.

\subsubsection{Time Stamping}
The digital setup needs a method for providing a time stamp for each pulse from the detectors. A global time stamp is provided by the digitizer for each acquired waveform or ``event". However, each event is 1204 ns long, and the pulses do not begin at the exact same point in time within the acquisition window. This is because the trigger clock triggers on the leading edge of pulses and runs at only \SI{125}{MHz}, whereas the sampling rate is \SI{1}{GHz}. Consequently, it was necessary to precisely determine where in the sampling window the pulse was located. For this purpose, a software-based CFD algorithm was implemented. The algorithm searched the first \SI{40}{ns} before the maximum amplitude of the pulse for the first sampling point to rise to 30\% of the maximum amplitude. Linear interpolation between this sampling point and the previous sampling point enabled a time stamp with resolution better than \SI{1}{ns} to be generated. In Fig.~\ref{fig:cfd_trig}, four pulses from the NE213 detector are plotted centered around their CFD trigger points. Although this time stamp is given with sub-ns precision, the accuracy is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate, resolution and bandwidth. 

\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/goodevents.pdf}
        \caption[Relative timing for NE213 detector pulses of different amplitudes.]{Relative timing for NE213 detector pulses of different amplitudes. A CFD algorithm was used to generate a precise time stamp. A CFD trigger level of 30\% of the maximum amplitude was used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsubsection{Data Selection}
As the digitizer only enforced a threshold, additional selection criteria were applied offline to filter the data set. Since the VX1751 triggered on all enabled channels simultaneously, all empty acquisition windows were discarded. This was done by determining and subtracting a baseline for each waveform. The baseline was determined by averaging over the first \SI{20}{ns} of the waveform. The peak amplitude relative to this new baseline was then determined. An amplitude threshold of \SI{24.4}{mV} was enforced removing all pulses of amplitude below this value (negative polarity pulses).

During the digitizer configuration, a baseline offset that was too high was inadvertently applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. Events whose amplitude could not be contained in the available range had their peaks clipped. These 5.7\% of the data were not discarded, but affected the QDC spectrum. The pulse-height spectrum shown in Fig.~\ref{fig:cutoffevents} highlights the clipped events in green.
\begin{figure}[hb!]
    \centering
        \includegraphics[width=0.9\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption[Digitized pulse-height spectrum from the NE213 detector.]{Digitized pulse-height spectrum from the NE213 detector. Clipped events which reached the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}

Certain events were removed because they caused either the baseline determination, pulse integration or CFD algorithm to produce spurious results. For example, in the case of 2.7\% of all events above trigger threshold, the baseline determination was deemed unsteady, see Fig.~\ref{fig:badevents}~(a). Events were removed when the standard deviation of samples in the baseline-determination window was greater than \SI{2}{mV}. A subset of the times this happened was because a pulse was located inside the baseline-determination window as shown in Fig.~\ref{fig:badevents}~(b). This was identified when the CFD trigger point was located within the first \SI{20}{ns} and happened 0.0016\% of the time. Figure~\ref{fig:badevents}~(c) shows a situation where the CFD triggered so late in the acquisition window that not enough samples followed to carry out the \SI{500}{\nano\second} LG integration. This occurred in 1.1\% of the events. And finally, 0.00066\% of the events were filtered because a peak was immediately  preceded by a smaller peak, see Fig.~\ref{fig:badevents}~(d). Since the CFD algorithm searched the \SI{40}{ns} immediately prior to the peak amplitude, it may have triggered due to the preceeding smaller pulse. All of the identified types of problematic events are summarized in Table~\ref{tab:badevents}.

\begin{table}[]
\center
\begin{tabular}{|l|l|l|}
\hline
Description                          & Percentage of events & Discarded \\ \hhline{|=|=|=|}
Pulses were clipped                           & 5.7\%                          & No                 \\ \hline
Unstable baseline                             & 2.7\%                        & Yes                \\ \hline
CFD trigger in baseline determination window  & $\ll$0.1\%                      & Yes                \\ \hline
CFD trigger too late for long-gate integration & 1.1\%                         & Yes                \\ \hline
CFD algorithm failed                          & $\ll$0.1\%                    & Yes                \\ \hline

\end{tabular}
\caption[Summary of problematic events.]{Summary of problematic events. In general the dataset was very healthy.}
\label{tab:badevents}
\end{table}

\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.87\textwidth]{DigitalSetup/badevents.pdf}
        \caption[Examples of rejected digitized events.]{Examples of rejected digitized events. Regions-of-interest are highlighted with red boxes. (a) The baseline was deemed unstable. (b) A pulse was located inside the baseline-determination window (a subset of (a)). (c) Less than \SI{500}{ns} follow the CFD trigger point, so the LG integration could not be carried out. (d) Closely adjacent pulses caused the CFD algorithm to fail.}
    \label{fig:badevents} 
\end{figure}
\newpage
\subsubsection{Energy Calibration}\label{sec:Ecal_D}
The energy calibration of the digital setup was carried out in a manner similar to the analog setup, using the Knox method. The pulses were integrated digitally over the exact same gate lengths used in the analog setup, namely 60 and 500 ns starting 25 ns before the CFD trigger. A major difference compared to the analog setup was in the baseline determination. For the analog setup, the pedestal was needed by the energy calibration to account for and subtract any baseline offset. It acted as a global baseline subtraction. This was not necessary in the digital setup since the baseline was subtracted on an event-by-event basis during the initial data processing. 

Both the Compton edges corresponding to \SI{2.23}{\MeV} and \SI{4.44}{\MeV} gamma-rays produced by the Pu/Be source as well as a Compton edge corresponding to \SI{1.33}{MeV} gamma-rays from a $^{60}$Co source were used for the calibration. The calibration points are listed in Table~\ref{tab:knox_d}. In Fig.~\ref{fig:D_QDC}, the Compton edges corresponding to \SI{1.33}{\MeV}, \SI{2.23}{\MeV} and \SI{4.44}{\MeV} gamma rays are marked in red, purple and orange respectively. The $^{60}$Co source produces gamma-rays of \SI{1.17}{\MeV} and \SI{1.33}{\MeV}, but due to energy resolution only the Compton edge corresponding to \SI{1.33}{MeV} is visible.  As with the analog setup the uncertainty is the greatest for the \SI{2.23}{\MeV} Compton edge.
\begin{table}[hb]
	\center
	\begin{tabular}{|l|l|l|l|}
	\hline
	channel (mV$\cdot$ns)            & 3791.4 & 7079.6  & 15480.9  \\ \hhline{|=|=|=|=|}
	$E_\gamma$ (MeV)          & 1.33    & 2.23  & 4.44 \\
	\hline
	$(E_{e})_{max}\; (\textrm{MeV}_{ee})$ & 0.96    & 2.00  & 4.20 \\
	\hline
	\end{tabular}
	\caption[Digital energy-calibration data.]{Digital energy-calibration data. Digitizer pulse integration channels fitted to known Compton edges. See text for details.}
	\label{tab:knox_d}
\end{table}
The baseline shift on the NE213 channel offset was set too high, restricting the range available to negative pulses to only \SI{0.6}{V}. This affected the Compton edge of the \SI{4.44}{\MeV} gamma-ray. In spite of this problem, Fig.~\ref{fig:D_QDC} shows that the calibration points still follow a linear trend within uncertainty. The fit parameters were used to produce the calibrated x-axis in the upper panel.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.8\textwidth]{DigitalResults/Ecall.pdf}
        \caption[Energy calibration of the digital setup.]{Energy calibration of the digital setup. The Compton edges corresponding to the \SI{1.33}{\MeV} gamma-ray from $^{60}$Co and the \SI{2.23}{\MeV} and \SI{4.44}{\MeV} gamma-rays from the Pu/Be source have been used to perform the energy calibration.}
    \label{fig:D_QDC}
\end{figure}

\subsubsection{Charge-Comparison Pulse-Shape Discrimination}
The CC method was implemented in the digital setup by integrating the waveforms over \SI{60}{ns} and \SI{500}{ns} gates respectively, starting \SI{25}{ns} before the CFD trigger. The exact same gate lengths and timing employed by the analog setup were used here to facilitate a direct comparison of the two setups. In addition, the separation between neutron and gamma-ray distributions was linearized by fine tuning the parameters $a$ and $b$ as shown in Fig~\ref{fig:psd_sketch}.

\section{Convolutional Neural Network}\label{sec:cnn}
The biggest advantage a digitizer offers is that it records the entire pulse rather than just extracting a few parameters from it. With the entire waveform available, PSD can be approached in ways that are not feasible with analog electronics. One such approach is to apply an artificial neural network to the task of discriminating between neutrons and gamma-rays. 
Artificial neural networks are function approximators which use learned parameters called weights to perform a specified task. There are a variety of different network types. For image classification tasks such as PSD, convolutional neural networks (CNNs) set the gold standard. Training neural networks to perform neutron/gamma-ray PSD is not a new approach. It has been successfully implemented in a number of studies for various scintillators, see Ref.~\cite{Griffiths}. The key difference between what has previously been done and what is done within this thesis lies in the manner the training data were selected.

\subsection{Training Dataset}
To train a network to discriminate between neutrons and gamma-rays, a set of digitized waveforms along with labels defining the species of each waveform is needed. One approach would be to create a precise simulation and train the network on the simulated data. Then the labels are known with certainty to represent the waveforms to which they are assigned. On the other hand, the simulated data must be an excellent representation of actual detector signals. This also implies that new simulations will be needed for new detectors. Another approach is to use different PSD techniques for labeling data as either neutrons or gamma-rays. This is the approach taken by  Griffiths et~al.~\cite{Griffiths}. They label their training data by plotting the number of samples within a given pulse that surpasses a certain threshold as a function of peak amplitude, and then making a cut to separate neutrons from gamma-rays. This approach is effective as long as the model used to generate the training data is not systematically mislabeling a certain type of pulses, such as pulses in a certain energy range. 

The approach taken here has been to take advantage of the extra information given by the ToF spectrum. The ToF spectrum provides access to a labeled set of neutron and gamma-ray pulses in the form of the neutron bump and the gamma-flash, see Fig~\ref{fig:CNN_data}. The downside of this approach is that the ToF spectrum will also contain random coincidences, which means that the training data will contain some neutrons mistakenly labeled as gamma-rays and vice versa. Since the background is composed of random coincidences and makes up only a small amount of the total training data, it is anticipated that the false neutrons/gamma-rays will not cause the network to systematically misclassify, but merely slow down the training.

\begin{figure}[h!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_data.pdf}
        \caption[Selection of training and validation data using ToF information.]{Selection of training and validation data using ToF information. Gamma-flash events are labeled 0 and events from the neutron bump are labeled 1.}
    \label{fig:CNN_data} 
\end{figure}

\subsection{How It Works}
CNNs use kernels of weights to extract features from an input image. The essential features of a kernel are the size, the stride and the weights. Figure~\ref{fig:CNN_frame1} shows an example of a kernel being applied to an input vector $\vec{x}$. When applying the network to PSD, $\vec{x}$ will be a digitized current pulse from the NE213 detector. In Fig. \ref{fig:CNN_frame1}, the convolution is performed with stride 2, i.e. the kernel is moved across the input in steps of 2. 
The kernel takes a single input vector and operates on 3 elements at a time, so it has dimensions 1$\times$3. The elements $\omega_i$ of the kernel are called weights. The kernel is scanned across $\vec{x}$ and a new vector or feature map $\vec{h}$ is produced. Each element of $\vec{h}$ is provided by a function $\phi$ which takes the scalar product of the kernel and a segment of $\vec{x}$ along with a bias $b$ as input, see Fig.~\ref{fig:CNN_frame1}. $\vec{\omega}$ and $b$ are the parameters which will be optimized through training.
A common choice of activation function for CNNs is the rectified linear unit, ReLU given by:
\begin{equation}
	\textrm{ReLU}(x) = 
	\begin{cases}
    	0, & \text{if}\ x<0 \\
    	x, & \text{otherwise}.
    \end{cases}
\end{equation}

%talk about act function

\begin{figure}[h!]
    \centering
        \includegraphics[width=0.85\textwidth]{DigitalSetup/CNN_frame1.pdf}
        \caption[Convolution of a vector and a kernel.]{Convolution of a vector and a kernel. Connections from the bias node are marked with a dashed line.}
    \label{fig:CNN_frame1} 
\end{figure}

Convolutional neural networks typically employ multiple layers each containing multiple filters, see Fig.~\ref{fig:CNN_frame2}. Note that Fig~\ref{fig:CNN_frame2} is drawn to illustrate how a CNN works and does not reflect the exact architecture of the network employed in this thesis.  The first operations carried out in this network is marked with \circled{1} and are convolutions. Three kernels are applied to the input vector to produce three new feature maps in exactly the same manner as was shown in Fig.~\ref{fig:CNN_frame1}. The job of  each filter is to highlight simple features in the input vector.

The next operations labeled \circled{2} are also convolutions. This time four kernels are scanned across the red, green and blue feature maps from the preceeding layer. Each of the four filters have dimension $3\times2$, meaning that they operate on two elements in each feature map simultaneously, in order to extract more complex information. This produces the four new feature maps $\vec g_1$, $\vec g_2$, $\vec g_3$, $\vec g_4$. The next operation labeled \circled{3} is a flattening of the four feature maps into a single vector $\vec f$. In the final operation labeled \circled{4}, the output $y$ of the network is given by an activation function $\phi_{\textrm{out}}$, which takes a linear combination of the elements of $\vec f$ as input. For binary classification problems, a common choice of activation function $\phi_{\textrm{out}}$ is the logistic function, which is bounded between 0 and 1:
\begin{equation}
	\phi_{out}(x) = \frac{1}{1+e^{-x}}.
\end{equation}
This function allows the output to be interpreted as a probability of the input waveform $\vec{x}$ representing a neutron.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_frame2.pdf}
        \caption[Two-layer CNN.]{Two-layer CNN. Each of the feature maps in the convolutional layers are produced with a unique kernel, and has been given its own color. For readability, only connections to the red and pink feature maps are shown. The other feature maps are connected in the same manner. Bias nodes have also been omitted for readability.}
    \label{fig:CNN_frame2} 
\end{figure}

The output of the network will be useless unless the weights have been properly trained to extract relevant features. The network is trained through a simple yet incredibly powerful method called ``backpropagation". In backpropagation, the derivative of an error function with respect to each weight in the network is found through repeated use of the chain rule. These derivatives are then used to make adjustments to the weights in order to minimize the error function.
The error function used in this work is the binary cross-entropy error function. It is commonly applied to binary classification tasks and is given by:
\begin{equation}
	E = -\frac{1}{N}\sum_n^N\left(	d_n\log(y(\vec x_n))+(1-d_n)\log(1-y(\vec x_n))	\right).
\end{equation}
This function calculates the average error $E$ over $N$ input vectors $\vec x_n$ with labels $d_n$. $y_n$ is the output of the network. In the case of neutron/gamma-ray discrimination, each vector $\vec x_n$ will be a digitized waveform. If $\vec x_n$ represents a neutron, then $d_n=1$ and the second term disappears. If instead $\vec x_n$ represents a gamma-ray, then $d_n=0$ and the first term disappears.
After propagating $N$ waveforms through the network, the weights can be updated using the derivatives of the error function. A simple updating method is the stochastic gradient descent updating rule:
\begin{equation}
	\omega^{t+1}_i = \omega^{t}_i - \eta\frac{\partial E^t}{\partial \omega_i},
\end{equation}
where $\omega_i$ is weight $i$ in the network, $E$ is the error function and $t$ is the training iteration. The constant $\eta$ is a scaling factor called the ``learning rate". It scales the corrections down to avoid overshooting optimal weights. The procedure is repeated until the error function has converged at a minimum value.




\subsection{Implementation of the Network}
A CNN was implemented using \textsc{Keras} version 2.2.4\footnote{\textsc{Keras} is a high-level framework for constructing deep-learning models in \textsc{Python}. It can be run using different backends for carrying out operations on tensors. Here \textsc{Tensorflow} 1.12.0 was used as a backend.}~\cite{keras}. The main features of the chosen architecture are summarized in Table~\ref{tab:architecture}. In addition to convolutional layers, this network also contains ``max pooling" layers. Max pooling layers reduce the size of each feature map individually by passing on only the maximum value of a given neighbourhood of the input vector. With a size of 2 and stride of 2, input vectors are reduced to half their size, see Fig.~\ref{fig:maxpooking}. Due to the kernel size and stride as well as the max pooling, each node in the flattened layer is indirectly connected to a large number of samples in the input layer. Both convolutional layers employ the ReLU activation function, while the final layer applies the logistic function.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.25\textwidth]{DigitalSetup/maxpooling.pdf}
        \caption[The Max pooling principle.]{The Max pooling principle.}
    \label{fig:maxpooking} 
\end{figure}

\begin{table}[h]
\center
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{Input layer: dimension 1$\times$300}                                            \\ \hline
Hidden layers       & Kernel Dimension & N kernels & Stride & Activation function \\ \hline
Convolutional layer & 9$\times$1           & 10                & 4             & ReLU                \\ \hline
Max pooling          & 2$\times$1           & -                 & 2             & -                   \\ \hline
Convolutional layer & 5$\times$10           & 16                & 2             & ReLU                \\ \hline
Max pooling          & 2$\times$1           & -                 & 2             & -                   \\ \hline
\multicolumn{5}{|c|}{Fully connected: dimension 1$\times$ 144}
\\ \hline
\multicolumn{5}{|c|}{Output layer: dimension 1$\times$1, activation function: 	$\phi_{out}(x) = \frac{1}{1+e^{-x}}$.
}               \\ \hline
\end{tabular}
\caption{Parameters essential to the CNN.}
\label{tab:architecture}
\end{table}



\begin{figure}[ht!]
    \centering
        \includegraphics[width=0.85\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption[Training and validation accuracy of the CNN.]{Training and validation accuracy of the CNN. The dashed line indicates the epoch corresponding to the chosen model.}
    \label{fig:CNN_training} 
\end{figure}

The network was trained on events from the gamma-ray and neutron ToF peaks of a 75 minute data set. This data-set was not used for any further analysis. 300 samples (or equivalently ns) from each pulse were used starting \SI{20}{ns} prior to the CFD trigger point. 75\% of the pulses or 3018 events from the neutron bump and 3018 events from the gamma-flash were used as labeled training data while the remaining 25\% were used to evaluate the model, see Fig.~\ref{fig:CNN_training}. Here the accuracy of the model, defined as the fraction of correctly labeled events, is plotted as a function of the epoch, with epoch defined as the number of times the network has trained on the entire training dataset. The blue curve shows the fraction of correctly labeled pulses achieved on the training data as a function of iteration, while the red curve shows the same thing on the validation data. The red curve varies more since it represents a smaller dataset. As both training and validation data contain some fraction of incorrectly labeled background events, the model is not expected to reach 100\% accuracy on either data set. This plot shows that although the performance on the training set keeps increasing, the performance on the validation set quickly levels out. For this reason, the model achieved at iteration 53 is used. The network is still learning beyond iteration 53, but it is no longer learning features that generalize to the validation data. It is instead overfitting to noise in the training data.


\subsection{Decision Study}
One way to gain a deeper understanding of how the CNN distinguishes between neutrons and gamma-rays is to examine the pulses classified with a high level of confidence. Figure \ref{fig:CNN95} shows examples of pulses classified with more than 95\% confidence. The pulses labeled as neutrons decay more slowly than those labeled as gamma-rays. This is in agreement with expectations as fast neutrons give rise to recoiling protons in the NE213. These recoiling protons tend to activate slow scintillation-light components more than electrons do. 

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{DigitalResults/neutronpulses.pdf}
  \caption{Neutron pulses.}
  \label{fig:CNNneutron}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{DigitalResults/gammaRayPulses.pdf}
  \caption{Gamma-ray pulses.}
  \label{fig:CNNgamma}
\end{subfigure}
\caption{Pulses classified by the CNN with more than 95\% confidence.}
\label{fig:CNN95}
\end{figure}

Examining pulses the network had trouble with can further explain the decision making process. The CNN was designed to assign a number between 0 and 1 to pulses, with values close to 0 meaning the network is certain it is looking at a gamma-ray and values close to 1 meaning that it is certain that it is looking at a neutron. 
Figure \ref{fig:occl} shows a pulse with a prediction value of 0.49, meaning that the network has difficulty determining particle species. By forcing a subset of samples in this waveform to zero, the decision of the network is constrained to the remaining information. The yellow curve in Fig. \ref{fig:occl} shows the network prediction when a 31 pixel window of zeros or ``blanks'' is scanned across the pulse. A prediction at a time $t$ in Fig. \ref{fig:occl} represents the prediction of the network when the 31 samples in the pulse centered around $t$ are set to zero. When blanking the body of the pulse, the prediction rises to around 1, which means the network is convinced it is looking at a neutron, based upon the remaining information in the tail. When the first part of the tail is blanked, the network becomes convinced that it is looking at a gamma-ray, based upon the remaining information. The network has learned that neutrons have a significant fraction of the total charge in the tail of the pulse and that gamma-rays have most of the charge in the body of the pulse.



\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalResults/ambiguouspulse.pdf}
        \caption[CNN decision study.]{CNN decision study. The green curve (left y-axis) shows a pulse the network could not clearly classify as either neutron or gamma-ray. The yellow curve (right y-axis) shows how the CNN prediction varies when part of the pulse is blanked.}
    \label{fig:occl} 
\end{figure}






\end{document}