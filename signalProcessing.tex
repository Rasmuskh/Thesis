\documentclass[main.tex]{subfiles}
\begin{document}

\section{Digital signal processing}
\subsection{Digitizer Specifications and Configuration}
Using a digitizer offers the freedom to carry out the processing on an event by event basis offline, making it possible to process the same dataset in multiple ways. On the downside discretizing a continuous signal means that you are limited by the sampling rate, resolution and bandwidth of the digitizer.

The digitizer used in this work is an 8 channel CAEN VX1751 digital waveform digitizer. It has a sampling rate of 1 GS/s in standard mode, but can also be operated in Double edge sampling mode, which disables 4  cannels but increases the sampling rate to 2 GS/s\cite[p.9]{CAEN}. The data presented in this thesis was acquired in standard mode. This means that we get one data point per nanosecond when digitizing signals. Given a set of samples there will always be an infinite number of waveforms that fit the points. This is called aliassing. However, if the sampling rate is at least twice as large as the highest frequency component of your signal then aliassing is avoided. This is known as the Nyquist Theorem. Ideally though, you will want even higher sampling rate in order to give a nice representation of the shape of pulses\cite{Spectrum}. %Elaborate

In order to reproduce signals with a high fidelity it is not enough to have a high sampling rate. The resolution and bandwidth also need to be considered. The VX1751 digitizer has a 1 V dynamic range, which means that the difference between maximum and minimum voltage is 1 V. How this range is used is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bit, so the 1 V dynamic range is cut in 1024 pieces each of size 0.978 mV\cite{CAEN}. 

The VX1751 has a bandwidth of 500 MHz. The bandwidth is important because it determines the frequency range of signals that can be read without significant attenuation. If the frequency of a signal is too high, then the amplitude recorded by the digitizer will be lower than the actual signals amplitude. The Bandwidth, $B$, is related to the rise time (the time it takes the pulse to rise from 10 to 90\% of peak amplitude), $T_{rise}$, through the following formula\cite[p.354]{Leo}
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{rise}
\end{equation}
Equation \ref{eq:bandwidth} tells us that an analog signal with a 1 ns risetime would need a 350 MHz bandwidth digitizer in order for the digitized signal to only be attenuated to 70\% of the analog signals amplitude. The typical rise time of the signals studied in this work is around 5-15 ns. So Bandwidth is not a major restriction.

The digitizer is controlled via CAEN's opensource software WaveDump\cite{WaveDump}. WaveDump configures the digitizer according to a text file supplied by the user. The number of datapoints per trigger is defined globally for all enabled channels. 
For each enabled channel the signal polarity, trigger threshold and baseline offset needs to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a signal must have in order for an event to be recorded. It is of note that the digitizer always triggers on all channels simultaneously. Since WaveDump writes all events to disc the filesize will grow quickly when multiple channels are enabled. 

The baseline offset is given in percent and determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset.

In the data presented in this thesis the NE213 detector was connected to channel 0 and a YAP detector connected to channel 1, and assigned negative polarity. A threshold of 48.9 mV (50 ADC) was applied to the NE213 and a threshold of 9.78 mV (10 ADC) was applied to the YAP channel. A baseline offset of 10\% was applied to the YAP channel and an offset of 40\% was accidentally applied to the NE213 channel. This meant that high amplitude pulses were cut, which as will be shown in a later chapter affected the energy calibration.



\subsection{Timestamping and Time of Flight Algorithms}
In order to get a good time of flight spectrum we need to have a precise timestamp for each pulse from the detectors. A general timestamp is provided by the digitizer for each event, but each event is 1204 ns long, and the pulse rise does not land in the exact same bin on each acquisition. For this reason a software based constant fraction discriminator was implemented. This algorithm looks at the first 40 ns before the peak amplitude and finds the first sampling point to rise above  a certain fraction of the peak amplitude (In this work 30\% was used). Using linear interpolation between this point and the previous point a better timestamp is generated in picoseconds. In figure \ref{fig:cfd_trig} 4 pulses from the NE213 detector have been plotted centered around their cfd trigger points. Although this timestamp is in picoseconds the resolution is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate resolution and bandwidth. 

Two different algorithms were developed for doing time of flight correlations; a multi-hit and a single-hit algorithm. The multi hit algorithm allows for each NE213 event to be Correlated with any gamma event within a chosen time window of itself. The timedifference is then binned in a histogram. 
The single hit algorithm makes use of column based operations on the dataframes and calculates time differences between neighbouring rows. Time differences are only kept for NE213 events and only if the neighbour was a YAP event. This approach is effective as long as you know that you are only interested in events one sampling window (1204 ns) or less away. This is true when tagging fast neutrons at the Aquarium since both neutron and gamma peak fit in a 100 ns window. 

The multi hit and the single hit algorithms yielded the same results in the region 0-500 ns for the data presented in this thesis. However, for future thermal neutron tagging experiments this will not be the case, since the time of flight will be greater than the acquisition window.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/goodevents.pdf}
        \caption{A cfd algorithm is used to generate precise timestamp. Here a cfd trigger of 30\% is used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsection{Data Processing}
Since WaveDump triggers on all enabled channels simultaneously it is neccesary to throw away all empty events. This is done by first determining and subtracting a baseline by averaging over the first 20 ns of each pulse. After that the amplitude relative to the new baseline is found and a chosen threshold is reinforced, by removing all pulses of amplitude below the threshold.

Certain events are removed because they are too problematic. For example 2.69\% of all events (not counting events below trigger threshold) the baseline determination fails as shown in \ref{fig:badevents} (A). This happens when the standard deviation over the baseline determination window is above 2 mV. 
A subset of the times the baseline determination fails will be due to a pulse rising inside the baseline determination window as shown in fig \ref{fig:badevents} (B). This can be recognized from the location of the CFD trigger and happens happens 0.0053\% of the time. Events are also removed if they occur too late in the acquisition window. Figure \ref{fig:badevents} (C) shows a situation where the cfd trigger occurs inside the longgate integration window, which will have an adverse effect on the QDC spectrum and pulse shape discrimination. This occers in 1.27\% of the events.

Occasionally, 0.00079\% of the events, events are filtered out because a peak is preceded by a smaller peak causing the CFD algorithm to trigger inside the smaller pulse. Eaxamples of this is shown in fig \ref{fig:badevents} (D).
\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/badevents.pdf}
        \caption{Since The NE213 detector is used for pulseshape discrimination it is important that (A) the the baseline is stable, (B) that the pulse rise does not land in the baseline determination window (a subset of (A)), (C) the tail of the pulse is contained. (D) Moreover Closely adjacent pulses may on rare occasions cause the cfd algorithm to find a slope of the wrong sign.}
    \label{fig:badevents} 
\end{figure}

During the digitizer configuration a too high baseline offset was applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. Consequently 

\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption{Digitized pulse height spectrum from the NE213 detector. Events reaching the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}


\subsection{Pulse Shape Discrimination}
Two approaches to pulse shape discrimination were explored with the digitizer. Firstly the charge comparisson method was implemented by integrating the waveforms over 60 and 500 nanoseconds respectively. Other gate lengths were also attempted, and actually it seemed beneficial to keep both somewhat shorter. However, to make the comparisson to the analog setup more direct the same gate lengths were used. In addition to this the separation between neutron and gamma distributions were 
\subsubsection{convolutional Neural Network Based Pulse Shape Analysis}
The biggest advantage a digitizer offers is that we don't just extract a few parameters from the pulses, but actually record the entire pulses. With the entire digitized waveform at our disposal we can approach pulse shape discrimination in ways that are not feasible with analog electronics. Neural networks are known for being able to approximate any continuous function provided they receive sufficient data to train on and a sufficiently complex architecture, so with a labelled set of data it should be possible to train a network to discriminate neutrons from gammas. It is particularly interesting because neutron tagging offers a relatively clean labeled dataset right out the box in the form of the neutron and and gamma peaks in the time of flight spectrum. 

A digitized pulse can be seen as a 1-dimensional image of a pulse, where each element is a discrete aproximation to the original analog signal. Training a network classify pulses between gammas and neutrons will then require the network to learn which features are relevant for the qualification. One approcah is to construct a multi layer perceptron. 

For a binary classification problem like neutron-gamma discrimination, this type of network will contains a layer of input nodes, with each node taking an element of the waveform as input, followed by one or more hidden layers and finally a single output node. Since each node is connected to every node in the following layer via a weight parameter, this type of model with have a large amount of parameters to train. An attractive alternative is to instead use a convolutional neural network.

Convolutional neural networks
\cite{Goodfellow-et-al-2016}

---
Data selection

\subsubsection{Arcitecture and Training}
The network described in this section was trained on digitized waveforms which were located within the neutron and gamma peaks of a time of flight spectrum. The dataset was approximately 75 minutes long and 75\% of the set was used for training while the remaining 25\% was used for validation. A different 10 minute dataset was used for testing the trained model before it was applied to the final dataset used throughout this thesis. 

The network architecture was found through trial and error. The model needed to be complex enough to learn to recognize the particles even at low energies but not needlessly complex as that might lead it to find patterns specific only to the training data as well as make applying the model to new data more computationally expensive. It was found that two convolutional layers each with 16 kernels of size 13 and stride 7 was effective when employed together with 2 maxpooling layers of kernel size 2 and stride 2. In order two avoid overtraining an 8\% dropout probability was applied to each convolutional layer.
The network employed here concisted of an input layer taking a a 300 sample long 1-tensor as input, followed by two convolutional layers each of which were followed by max pooling layers, the output of the last maxpooling layer is a flattened array, in which each element or node is connected to the output node.

Previously the hyperbolic tangens function, $\tanh$, and the logistic function, $\sigma$, were common choices for activation functions in the hidden layers of neural networks.
\begin{equation}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}
\begin{equation}
\frac{d\tanh(x)}{dx}= 1 -tanh(x)^2
\end{equation}
\begin{equation}
	\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
\begin{equation}
\frac{d\sigma}{dx} = \sigma(x)(1-\sigma(x))
\end{equation}
These functions however have derivatives bounded in the ranges [0, 1] and [0, 0.25], which means that errors propagated through the network with repeated used of the chain rule will approach zero as one propagates errors from the later layers to the earlier ones. This means that later layers will train faster than earlier ones. To avoid this the ReLu function can be used, where ReLu is given by:
\begin{equation}
	Relu(x) = max(0,x)
\end{equation}
%dead neurons
In the final layer of the network the activation function is the logistic function. Since this function is bounded between zero and 1 it is ideal for binary classification, letting outputs closer to 1 represent neutrons and those closer to zero represent gammas.
Layer (type)                 Output Shape              number of parameters   
\newline =================================================================
\newline conv1d (Conv1D)            (None, 98, 28)            224       
\newline dropout (Dropout)          (None, 98, 28)            0         
\newline maxpooling1d (MaxPooling1 (None, 49, 28)            0         
\newline conv1d (Conv1D)            (None, 15, 28)            5516      
\newline dropout (Dropout)          (None, 15, 28)            0         
\newline maxpooling1d (MaxPooling1 (None, 7, 28)             0         
\newline flatten (Flatten)          (None, 196)               0         
\newline preds (Dense)                (None, 1)                 197       
\newline =================================================================
\newline Total params: 5,937
\newline Trainable params: 5,937
\newline Non-trainable params: 0
---
\subsubsection{Training}
In order to train the model we need a measure of how well or poorly it is performing. This is the loss function. The goal of the network will then be to minimize this function by updating the weigths. This is done by using an extremely powerful yet conceptually simple algorithm called \textit{Backpropagation}
-backprop
-overtraining
-datasets

Training dataset: Using data from time of flight peaks.
\newline test dataset
\newline validation set
\newline Application




\label{sec:cnn}
\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption{The validation and training accuracy plotted as a function of training iteration. The validation accuracy stops growing around iteration 230. Beyond iteration 400 we can see that the validation accuracy is slowly decreasing although the training accuracy is increasing. This is overtraining.}
    \label{fig:CNN_training} 
\end{figure}





\end{document}