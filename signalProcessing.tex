\documentclass[main.tex]{subfiles}
\begin{document}

\section{Digital signal processing}
\subsection{Digitizer Specifications and Configuration}
Using a digitizer offers the freedom to carry out the processing on an event by event basis offline, making it possible to process the same dataset in multiple ways. On the downside discretizing a continuous signal means that you are limited by the sampling rate, resolution and bandwidth of the digitizer.

The digitizer used in this work is an 8 channel CAEN VX1751 digital waveform digitizer. It has a sampling rate of 1 GS/s in standard mode, but can also be operated in Double edge sampling mode, which disables 4  cannels but increases the sampling rate to 2 GS/s\cite[p.9]{CAEN}. The data presented in this thesis was acquired in standard mode. This means that we get one data point per nanosecond when digitizing signals. Given a set of samples there will always be an infinite number of waveforms that fit the points. This is called aliassing. However, if the sampling rate is at least twice as large as the highest frequency component of your signal then aliassing is avoided. This is known as the Nyquist Theorem. Ideally though, you will want even higher sampling rate in order to give a nice representation of the shape of pulses\cite{Spectrum}. %Elaborate

In order to reproduce signals with a high fidelity it is not enough to have a high sampling rate. The resolution and bandwidth also need to be considered. The VX1751 digitizer has a 1 V dynamic range, which means that the difference between maximum and minimum voltage is 1 V. How this range is used is controlled through the choice of signal polarity and baseline offset. The resolution of the digitizer is 10 bit, so the 1 V dynamic range is cut in 1024 pieces each of size 0.978 mV\cite{CAEN}. 

The VX1751 has a bandwidth of 500 MHz. The bandwidth is important because it determines the frequency range of signals that can be read without significant attenuation. If the frequency of a signal is too high, then the amplitude recorded by the digitizer will be lower than the actual signals amplitude. The Bandwidth, $B$, is related to the rise time (the time it takes the pulse to rise from 10 to 90\% of peak amplitude), $T_{rise}$, through the following formula\cite[p.354]{Leo}
\begin{equation}
\label{eq:bandwidth}
B=0.35/T_{rise}
\end{equation}
Equation \ref{eq:bandwidth} tells us that an analog signal with a 1 ns risetime would need a 350 MHz bandwidth digitizer in order for the digitized signal to only be attenuated to 70\% of the analog signals amplitude. The typical rise time of the signals studied in this work is around 5-15 ns. So Bandwidth is not a major restriction.

The digitizer is controlled via CAEN's opensource software WaveDump\cite{WaveDump}. WaveDump configures the digitizer according to a text file supplied by the user. The number of datapoints per trigger is defined globally for all enabled channels. 
For each enabled channel the signal polarity, trigger threshold and baseline offset needs to be defined. The trigger threshold is the minimum amplitude relative to the baseline that a signal must have in order for an event to be recorded. It is of note that the digitizer always triggers on all channels simultaneously. Since WaveDump writes all events to disc the filesize will grow quickly when multiple channels are enabled. 

The baseline offset is given in percent and determines where in the dynamic range of the digitizer the baseline is placed. A baseline offset of 0\% will cause the entire range to be used for pulses of the selected polarity. This means that undershoot will not be seen. Therefore it is best to use a small baseline offset.

In the data presented in this thesis the NE213 detector was connected to channel 0 and a YAP detector connected to channel 1, and assigned negative polarity. A threshold of 48.9 mV (50 ADC) was applied to the NE213 and a threshold of 9.78 mV (10 ADC) was applied to the YAP channel. A baseline offset of 10\% was applied to the YAP channel and an offset of 40\% was accidentally applied to the NE213 channel. This meant that high amplitude pulses were cut, which as will be shown in a later chapter affected the energy calibration.

The output from WaveDump was originally a csv file per channel, but the software was modified to output all data in a single file. Running with the PuBe source, the NE213 and a YAP produced data rates of around 120 GB per hour. Initially The Python library pandas was used for processing the data. This library allows the user to store data in a column based manner, which makes data analysis and filtering easier. Unfortunately Pandas was found to scale poorly with larger datasets, so instead the processing was done using DASK. Dask is built on top of Pandas but allows the user to process data in bites of a chosen size. Only the partitions currently being processed is pulled into memory. This is an advantage when the dataset is much larger than the available RAM. Since DASK oricesses data in bites it executes operations in a lazy manner, i.e. it does not carry out any operations untill the answer is explicitly needed. This avoids having to pull the same data into memory multple times. Another advantage of this library is that it by default runs on all available logical cores. The data processed with DASK was saved to parquet files, which could then be opened with Pandas. After processing and throwing away of events the output parquet file would have a size of around 7.3 GB. The major part of this filesize is due to the entire digitized waveforms.

The downside to using dask is that the syntax is a bit less pythonic, requiring you to avoid explicit loops and lacking a lot of features found in Pandas. Furthermore, when the data fits into memory Pandas will be faster than dask. Since the major part of the filesize was the waveforms, it was possible to use Pandas for most of the data analysis, by selecting which columns to read into memory.




\subsection{Timestamping and Time of Flight Algorithms}
In order to get a good time of flight spectrum we need to have a precise timestamp for each pulse from the detectors. A general timestamp is provided by the digitizer for each event, but each event is 1204 ns long, and the pulse rise does not land in the exact same bin on each acquisition. For this reason a software based constant fraction discriminator was implemented. This algorithm looks at the first 40 ns before the peak amplitude and finds the first sampling point to rise above  a certain fraction of the peak amplitude (In this work 30\% was used). Using linear interpolation between this point and the previous point a better timestamp is generated in picoseconds. In figure \ref{fig:cfd_trig} 4 pulses from the NE213 detector have been plotted centered around their cfd trigger points. Although this timestamp is in picoseconds the resolution is limited by the determination of the pulse amplitude, which in turn is limited by the sampling rate resolution and bandwidth. 

Two different algorithms were developed for doing time of flight correlations; a multi-hit and a single-hit algorithm. The multi hit algorithm allows for each NE213 event to be Correlated with any gamma event within a chosen time window of itself. The time difference is then binned in a histogram. 
The single hit algorithm makes use of column based operations on the dataframes and calculates time differences between neighbouring rows. Time differences are only kept for NE213 events and only if the neighbour was a YAP event. This approach is effective as long as you know that you are only interested in events one sampling window (1204 ns) or less away. This is true when tagging fast neutrons at the Aquarium since both neutron and gamma peak fit in a 100 ns window. 

The multi hit and the single hit algorithms yielded the same results in the region 0-500 ns for the data presented in this thesis. However, for future thermal neutron tagging experiments this will not be the case, since the time of flight will be greater than the acquisition window.
\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/goodevents.pdf}
        \caption{A cfd algorithm is used to generate precise timestamp. Here a cfd trigger of 30\% is used.}
    \label{fig:cfd_trig} 
\end{figure}

\subsection{Data Processing}
Since WaveDump triggers on all enabled channels simultaneously it is neccesary to throw away all empty events. This is done by first determining and subtracting a baseline by averaging over the first 20 ns of each pulse. After that the amplitude relative to the new baseline is found and a chosen threshold is reinforced, by removing all pulses of amplitude below the threshold.

Certain events are removed because they are too problematic. For example 2.69\% of all events (not counting events below trigger threshold) the baseline determination fails as shown in \ref{fig:badevents} (A). This happens when the standard deviation over the baseline determination window is above 2 mV. 
A subset of the times the baseline determination fails will be due to a pulse rising inside the baseline determination window as shown in fig \ref{fig:badevents} (B). This can be recognized from the location of the CFD trigger and happens happens 0.0053\% of the time. Events are also removed if they occur too late in the acquisition window. Figure \ref{fig:badevents} (C) shows a situation where the cfd trigger occurs inside the longgate integration window, which will have an adverse effect on the QDC spectrum and pulse shape discrimination. This occers in 1.27\% of the events.

Occasionally, 0.00079\% of the events, events are filtered out because a peak is preceded by a smaller peak causing the CFD algorithm to trigger inside the smaller pulse. Eaxamples of this is shown in fig \ref{fig:badevents} (D).
\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/badevents.pdf}
        \caption{Since The NE213 detector is used for pulseshape discrimination it is important that (A) the the baseline is stable, (B) that the pulse rise does not land in the baseline determination window (a subset of (A)), (C) the tail of the pulse is contained. (D) Moreover Closely adjacent pulses may on rare occasions cause the cfd algorithm to find a slope of the wrong sign.}
    \label{fig:badevents} 
\end{figure}

During the digitizer configuration a too high baseline offset was applied. This meant that only 60\% of the dynamic range was available for signals from the NE213 detector. Consequently 

\begin{figure}[ht!]
    \centering
        \includegraphics[width=1\textwidth]{DigitalSetup/cutoffevents.pdf}
        \caption{Digitized pulse height spectrum from the NE213 detector. Events reaching the limit of the dynamic range are highlighted in green.}
    \label{fig:cutoffevents} 
\end{figure}


\subsection{Pulse Shape Discrimination}
Two approaches to pulse shape discrimination were explored with the digitizer. Firstly the charge comparisson method was implemented by integrating the waveforms over 60 and 500 nanoseconds respectively, starting 25 nanoseconds before the CFD trigger. Other gate lengths were also attempted, and actually it seemed beneficial to keep both somewhat shorter. However, to make the comparisson to the analog setup more direct the same gate lengths were used. In addition to this the separation between neutron and gamma distributions were was linearized by adding baseline offsets to each of the charge integrals.
\subsection{convolutional Neural Network Based Pulse Shape Analysis}
The biggest advantage a digitizer offers is that we don't just extract a few parameters from the pulses, but actually record the entire pulses. With the entire digitized waveform at our disposal we can approach pulse shape discrimination in ways that are not feasible with analog electronics. Neural networks are known for being able to approximate any continuous function provided they receive sufficient data to train on and a sufficiently complex architecture. So with a labelled set of data it should be possible to train a network to discriminate neutrons from gammas. It is particularly interesting because neutron tagging offers a relatively clean labeled dataset right out the box in the form of the neutron and and gamma peaks in the time of flight spectrum. 

A digitized pulse can be seen as a 1-dimensional image of a pulse, where each element is a discrete aproximation to the original analog signal. Training a network classify pulses between gammas and neutrons will then require the network to learn which features are relevant for the qualification. One approcah is to construct a multi layer perceptron. 

For a binary classification problem like neutron-gamma discrimination, this type of network will contains a layer of input nodes, with each node taking an element of the waveform as input, followed by one or more hidden layers and finally a single output node. Since each node is connected to every node in the following layer via a trainable weight parameter, this type of model will have a very large amount of parameters to train. An attractive alternative is to instead use a convolutional neural network\cite{Goodfellow-et-al-2016}.

A convolutional neural network takes advantage of the fact that neighboring points in our waveform will have some relation relation. Some neighbouring points make up the sharp rising edge, some lie on the slope of the tail and some might just show the jittering noise of the background. By scanning smaller arrays or kernels across the original waveform we can generate a series of new images. Now the elements of the kernels are the trainale parameters and the goal is to train them to highlight useful features from the original waveform. The following convolution
---
Data selection

\subsubsection{Arcitecture}

The network architecture was to a large extent found through trial and error. The model needed to be complex enough to learn to recognize the particles even at low energies but not needlessly complex as that might lead it to find patterns specific only to the relatively small training data set as well as make applying the model to new data more computationally expensive. 

As input to the network a 300 ns of each digitized waveform was used, starting 20 ns before the CFD trigger. With a sampling rate of 1GS/s this gives and input vector of 300 samples.
\begin{table}[hb]
\center
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{\textbf{Input layer}: vector size 300}                                            \\ \hline
\textbf{Hidden layers}       & Kernel size & N kernels & Stride & Activation fcn \\ \hline
\textbf{Convolutional layer} & 9           & 12                & 5             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\textbf{Convolutional layer} & 9           & 12                & 5             & ReLU                \\ \hline
\textbf{Maxpooling}          & 2           & -                 & 2             & -                   \\ \hline
\multicolumn{5}{|c|}{\textbf{Output layer} size 1, \textbf{activation function}: Sigmoid}               \\ \hline
\end{tabular}
\caption{Table of the essential parameters in the network.}
\label{tab:architecture}
\end{table}

It was found that two convolutional layers each with 12 kernels was a nice compromise between vomplexity and simplicity. The first convolutional layer used kernel size 9 and stride 4, while the second kernel used size 5 and stride 2, and each convolutional layer was followed by a maxpooling layer of kernel size 2 and stride 2. All the output arrays from the last maxpooling layer are laid in a line to form a single vector, in which every element is connected to the output node.

Previously the hyperbolic tangens function, $\tanh$, and the logistic function, $\sigma$, were common choices for activation functions in the hidden layers of neural networks.
\begin{equation}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}
\begin{equation}
\frac{d\tanh(x)}{dx}= 1 -tanh(x)^2
\end{equation}
\begin{equation}
	\sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}
\begin{equation}
\frac{d\sigma}{dx} = \sigma(x)(1-\sigma(x))
\end{equation}
These functions however have derivatives bounded in the ranges [0, 1] and [0, 0.25], which means that errors propagated through the network with repeated use of the chain rule will approach zero as one propagates errors from the later layers to the earlier ones. This means that the earlier layers will train train slower than later ones or even fail to properly train at all. To avoid this the, rectified linear unit, ReLU, function can be used instead. It is defined as:
\begin{equation}
	ReLU(x) = max(0,x)
\end{equation}
%dead neurons
In the final layer of the network the activation function is the logistic function. Since this function is bounded between zero and 1 it is ideal for binary classification, letting outputs closer to 1 represent neutrons and those closer to zero represent gammas.
Layer (type)                 Output Shape              

\subsubsection{Training}
The network described in this section was trained on digitized waveforms which were located within the neutron and gamma peaks of a time of flight spectrum. The dataset was approximately 75 minutes long and 75\% of the set was used for training while the remaining 25\% was used for validation. Since the neutron peak is wider it will be more contaminated by gamma background, so the gamma labelled data also included a 6 ns wide window of random coincidence events. prior to This yielded 3131 neutrons and 3131 gammas. There was 3131 events in the neutron set and 3172 in the gamma set, to further minimize bias only 3131 events from the gamma set were used. A different 10 minute dataset were used for testing the trained model, before it was applied to the final dataset used throughout this thesis.

In order to train the model we need a measure of how well or poorly it is performing. This is the loss function. The goal of the network will then be to minimize this function by updating the weigths. This is done by using an extremely powerful yet conceptually simple algorithm called \textit{Backpropagation}.

Loss function: binary Cross entropy: 
\begin{equation}
E(\omega) = -\Sigma_{n=1}^N \Big(d_n\log y(\mathbf x_n) + (1 - d_n)\log(1-y(\mathbf x_n))\Big)
\end{equation}
Weight update rule:

\begin{equation}
\omega_{ij} \rightarrow \omega_{ij} - \eta\frac{\partial E}{\partial W_{ij}}
\end{equation}
-backprop
-overtraining
-datasets

\label{sec:cnn}
\begin{figure}[ht!]
    \centering
        \includegraphics[width=\textwidth]{DigitalSetup/CNN_training.pdf}
        \caption{The validation and training accuracy plotted as a function of training iteration. both curves have been smoothened using an averaging filter in order to make trends more visible.}
    \label{fig:CNN_training} 
\end{figure}





\end{document}